{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lG-ZzBIE_gs6"
   },
   "source": [
    "_This notebook contains all the sample code and solutions to the exercises in chapter 11._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "krhguu6b_gs7"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QsIljAgf_gs8"
   },
   "source": [
    "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "j5fNwTgY_gs9",
    "outputId": "ed6c620b-8695-45dd-cfd2-44d225377e0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "orTwF_oY_gtD"
   },
   "source": [
    "Deep neural networks have more than one hidden layes. As we add more layers, we may face several problems:\n",
    "    1. Vanishing gradients problem (or the related exploding gradients problem) that affects deep neural networks and makes lower layers very hard to train.\n",
    "    2. Slow Training\n",
    "    3. Risk overfitting the training set\n",
    "  We will look at these problem and try to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KlITb6BT_gtE"
   },
   "source": [
    "# Vanishing/Exploding Gradients Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zb1mSbC1_gtF"
   },
   "source": [
    "Backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient on the way. Once the algorithm has computed the gradient of the cost function with regards to each parameter in the network, it uses these gradients to update each parameter with a Gradient Descent step. <br>\n",
    "\n",
    "Unfortunately, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution. This is called the vanishing gradients problem. <br>\n",
    "\n",
    "In some cases, the opposite can happen: the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges. This is the exploding gradients problem, which is mostly encountered in recurrent neural networks. More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds. <br>\n",
    "\n",
    "This problem was largely unresolved until 2010 A paper titled “Understanding the Difficulty of Training\n",
    "Deep Feedforward Neural Networks” by Xavier Glorot and Yoshua Bengi found that random weight initialization using a normal distribution with a mean of 0 and a standard deviation of 1 fixes the problem. If this is not done, the variance is increases with each additional layer. <br>\n",
    "\n",
    "This is actually made worse by the fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent function has a mean of 0 and behaves slightly better than the logistic function in deep networks). <br>\n",
    "\n",
    "In Logistic activation function inputs may become large (negative or positive) and the function saturates at 0 or 1, with a derivative extremely close to 0. When backpropagation starts, it has little gradient to propagate back through the network, and this small gradient keeps getting diluted as backpropagation progresses down through the top layers, so there is really nothing left for the lower layers. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T-Wipr1g_gtG"
   },
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "colab_type": "code",
    "id": "jdrjIY6H_gtJ",
    "outputId": "eeb65d3f-de2c-4ee5-d63a-72f71279e750"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure sigmoid_saturation_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "save_fig(\"sigmoid_saturation_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nKeZCbi1_gtN"
   },
   "source": [
    "## Xavier and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2pav0xgu_gtO"
   },
   "source": [
    "The signal need to signal to flow properly in both directions: in the forward direction when making predictions, and in the\n",
    "reverse direction when backpropagating gradients without dying or saturation. <br>\n",
    "\n",
    "The proposed solution by Glorot and Bengio requires equal variance in inputs and outputs. It is actually not possible unless the layer has an equal number of input and output connections. But there is a good compromise: <br>\n",
    "\n",
    "The connection weights must be initialized randomly according to the following formula (He or Glorot initialization) of logistic activation function. <br>\n",
    "\n",
    "He for ReLU\n",
    "Normal distribution with mean 0 and standard deviation $\\sigma=\\sqrt{\\frac{2}{n_{inputs} + n_{outputs}}}$ <br>\n",
    "\n",
    "For uniform distribution $U(-r,r)$ with $r=\\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}}$ <br>\n",
    "\n",
    "When the number of of connections is equal it can be simplified to $\\sigma = \\sqrt{\\frac{1}{n_{inputs}}}$\n",
    "\n",
    "For None, tanh, logistic, softmax -- Glorot initialization\n",
    "\n",
    "$fan_{in} = N_{inputs} + N_{neurons}$ <br>$fan_{out} = N_{outputs} + N_{neurons}$ <br>$fan_{avg} = (fan_{in} + fan_{out})/2$ <br>\n",
    "The connection weights must be initialized randomly according to the following formula (Glorot initialization) of logistic activation function. <br>\n",
    "Normal distribution with mean 0 and standard deviation $\\sigma=\\sqrt{\\frac{1}{fan_{avg}}}$ <br>\n",
    "For uniform distribution $U(-r,r)$ with $r=\\sqrt{\\frac{3}{fan_{avg}}}$ <br>\n",
    "When the number of of connections is equal it can be simplified to $\\sigma = \\sqrt{\\frac{1}{fan_{avg}}}$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "LTwgbfjW_gtO",
    "outputId": "8b88dd6a-2ed8-4d78-ea29-51291312bc3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Constant',\n",
       " 'GlorotNormal',\n",
       " 'GlorotUniform',\n",
       " 'HeNormal',\n",
       " 'HeUniform',\n",
       " 'Identity',\n",
       " 'Initializer',\n",
       " 'LecunNormal',\n",
       " 'LecunUniform',\n",
       " 'Ones',\n",
       " 'Orthogonal',\n",
       " 'RandomNormal',\n",
       " 'RandomUniform',\n",
       " 'TruncatedNormal',\n",
       " 'VarianceScaling',\n",
       " 'Zeros',\n",
       " 'constant',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'glorot_normal',\n",
       " 'glorot_uniform',\n",
       " 'he_normal',\n",
       " 'he_uniform',\n",
       " 'identity',\n",
       " 'lecun_normal',\n",
       " 'lecun_uniform',\n",
       " 'ones',\n",
       " 'orthogonal',\n",
       " 'random_normal',\n",
       " 'random_uniform',\n",
       " 'serialize',\n",
       " 'truncated_normal',\n",
       " 'variance_scaling',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "[name for name in dir(keras.initializers) if not name.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OO9foGcm_gtR",
    "outputId": "9efd69ca-de32-440f-cef7-287653ced215"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fecf0253400>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QKXr0Bk5_gtT"
   },
   "source": [
    "We use *fan_avg* for Glorot initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7LhOU76F_gtT",
    "outputId": "e375bd50-027d-4e3b-8b79-93e83bcb6eeb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fecf038cb80>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',\n",
    "                                          distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QTBTsQJ_gtV"
   },
   "source": [
    "## Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZlunBC3M_gtV"
   },
   "source": [
    "Glorot and Bengio show that the vanishing/exploding gradients problems were in part due to a poor choice of activation function. Sigmoid function copied from biology is not a good choice. ReLU activation function performs much better, mostly it does not produce extreme values and is fast to compute. <br>\n",
    "\n",
    "\n",
    "ReLU wiht large learning rate suffers from a problem known as the dying neurons when they stop outputting anything other than 0. If the updated weights produce $z<0$ the ReLU function will return 0 and the gradiend will be also 0. This neuron is likely to stay dead. <br>\n",
    "\n",
    "There a few ways to solve this problem: <br>\n",
    "\n",
    "1. Leaky ReLU defined as $LeakyReLU \\alpha(z) = \\max(\\alpha z, z)$. The hyperparameter $\\alpha$ defines how much the function “leaks”: it is the slope of the function for z < 0, and is typically set to 0.01.  This small slope ensures that leaky ReLUs never die; they can go into a long coma, but they have a chance to eventually wake up. Leaky ReLU tends to outperfrom ReLU activation function. Large $\\alpha=0.2$ sometimes performs better than 0.01. \n",
    "\n",
    "2. Randomized Leaky ReLU (RReLU), where $\\alpha$ is picked randomly during training and fixed during testing. This randomization reduces overfitting.\n",
    "\n",
    "3. Parametric Leaky ReLU (PReLU) where we optimeze parameter $\\alpha$ during backpropagation. This methods overperforms other ReLU on large datasets, but tends to overfit smaller ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_Ib2ve-KRiP"
   },
   "source": [
    "Keras activation funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "TNiucpo0L_HS",
    "outputId": "68f363fd-0407-4318-a3a5-ed6123a13005"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deserialize',\n",
       " 'elu',\n",
       " 'exponential',\n",
       " 'get',\n",
       " 'hard_sigmoid',\n",
       " 'linear',\n",
       " 'relu',\n",
       " 'selu',\n",
       " 'serialize',\n",
       " 'sigmoid',\n",
       " 'softmax',\n",
       " 'softplus',\n",
       " 'softsign',\n",
       " 'swish',\n",
       " 'tanh']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(keras.activations) if not m.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_nMwMtaSL_qk",
    "outputId": "8a094744-02b4-4ab9-ab8b-432113cda441"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LeakyReLU', 'PReLU', 'ReLU', 'ThresholdedReLU']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Relue activation functions:\n",
    "[m for m in dir(keras.layers) if \"relu\" in m.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aVgu-Ld4_gtV"
   },
   "source": [
    "### Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4-p4SG-Sw7rr"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ud1WnYiNv6LF"
   },
   "source": [
    "Djork-Arné Clevert et al. the exponential linear unit (ELU) activation function, it tends to outperform other ReLU both in time and testing accuracy:\n",
    "$$ELU_{\\alpha}(z) = \\begin{cases} \\alpha(\\exp(z) -1) \\quad &if \\quad z<0 \\\\ z \\quad &if \\quad z \\geq 0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1xUjm7wbv6LF"
   },
   "source": [
    "It looks a lot like the ReLU function, with a few major differences:\n",
    "1. It take negative values when z < 0, which allows the unit to have an average output closer to 0 helping to alleviate vanishing gradients problem.\n",
    "2. The hyperparameter $\\alpha$ defines the value that the ELU function approaches when z is a large negative number. It is usually set to 1, but you can be tweaked.\n",
    "3. ELU has a nonzero gradient for $z < 0$, which avoids the dying units issue.\n",
    "4. ELU is is smooth everywhere, including around z = 0, which helps speed up Gradient Descent. <br>\n",
    "\n",
    "\n",
    "The main drawback of the ELU activation function is that it is slower to compute than the ReLU and its\n",
    "variants (due to the use of the exponential function), but during training this is compensated by the faster\n",
    "convergence rate. However, at test time an ELU network will be slower than a ReLU network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kzNzKuPR_gtW"
   },
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "colab_type": "code",
    "id": "oePPX88b_gtX",
    "outputId": "3951a69a-6353-4a55-81d3-61ac3de1bcab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure leaky_relu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "save_fig(\"leaky_relu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPv_g6C4_gtZ"
   },
   "source": [
    "Implementing Leaky ReLU in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "tBBCgasp_gta",
    "outputId": "f94ede2a-54aa-43e0-d39e-1929ecda55c7"
   },
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "-0OAg36NMcG3",
    "outputId": "53a08a22-5b3b-4712-ea71-4e0f11f173cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 1.2819 - accuracy: 0.6229 - val_loss: 0.8886 - val_accuracy: 0.7160\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 2s 992us/step - loss: 0.7955 - accuracy: 0.7362 - val_loss: 0.7130 - val_accuracy: 0.7656\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 2s 902us/step - loss: 0.6816 - accuracy: 0.7721 - val_loss: 0.6427 - val_accuracy: 0.7898\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 2s 930us/step - loss: 0.6217 - accuracy: 0.7944 - val_loss: 0.5900 - val_accuracy: 0.8066\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 2s 928us/step - loss: 0.5832 - accuracy: 0.8075 - val_loss: 0.5582 - val_accuracy: 0.8200\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 1s 851us/step - loss: 0.5553 - accuracy: 0.8157 - val_loss: 0.5350 - val_accuracy: 0.8238\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 2s 890us/step - loss: 0.5338 - accuracy: 0.8225 - val_loss: 0.5157 - val_accuracy: 0.8304\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 2s 895us/step - loss: 0.5173 - accuracy: 0.8273 - val_loss: 0.5078 - val_accuracy: 0.8284\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 2s 877us/step - loss: 0.5040 - accuracy: 0.8289 - val_loss: 0.4895 - val_accuracy: 0.8390\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 2s 910us/step - loss: 0.4924 - accuracy: 0.8321 - val_loss: 0.4817 - val_accuracy: 0.8396\n"
     ]
    }
   ],
   "source": [
    "# LeakyRelu model\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "# build the model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.LeakyReLU(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "# compile the model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "# Run the model:\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZwEaMi50_gtt"
   },
   "source": [
    "### ELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UlFSKyOc_gtu"
   },
   "source": [
    "Djork-Arné Clevert et al. the exponential linear unit (ELU) activation function, it tends to outperform other ReLU both in time and testing accuracy:\n",
    "$$ELU_{\\alpha}(z) = \\begin{cases} \\alpha(\\exp(z) -1) \\quad &if \\quad z<0 \\\\ z \\quad &if \\quad z \\geq 0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eYgkR-8f_gtu"
   },
   "source": [
    "It looks a lot like the ReLU function, with a few major differences:\n",
    "1. It take negative values when z < 0, which allows the unit to have an average output closer to 0 helping to alleviate vanishing gradients problem.\n",
    "2. The hyperparameter $\\alpha$ defines the value that the ELU function approaches when z is a large negative number. It is usually set to 1, but you can be tweaked.\n",
    "3. ELU has a nonzero gradient for $z < 0$, which avoids the dying units issue.\n",
    "4. ELU is is smooth everywhere, including around z = 0, which helps speed up Gradient Descent. <br>\n",
    "\n",
    "\n",
    "The main drawback of the ELU activation function is that it is slower to compute than the ReLU and its\n",
    "variants (due to the use of the exponential function), but during training this is compensated by the faster\n",
    "convergence rate. However, at test time an ELU network will be slower than a ReLU network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wg9hHSur_gtu"
   },
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "colab_type": "code",
    "id": "iVq4yylh_gtv",
    "outputId": "e8487130-fd3a-405f-8459-f5e7a738ab5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure elu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"elu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lR88rOh2_gtw"
   },
   "source": [
    "Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gXwICWj1_gtx",
    "outputId": "afb55cbd-0fc4-448f-ca6b-8ceb50273fac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fecf038f850>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"elu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HQBHuKV8_gty"
   },
   "source": [
    "### SELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wuzAtOJs_gtz"
   },
   "source": [
    "This activation function was proposed in this [great paper](https://arxiv.org/pdf/1706.02515.pdf) by Günter Klambauer, Thomas Unterthiner and Andreas Mayr, 2017. During training, a neural network composed exclusively of a stack of dense layers using the SELU activation function and LeCun initialization will self-normalize: the output of each layer will tend to preserve the same mean and variance during training, which solves the vanishing/exploding gradients problem. As a result, this activation function outperforms the other activation functions very significantly for such neural nets, so you should really try it out. Unfortunately, the self-normalizing property of the SELU activation function is easily broken: you cannot use ℓ1 or ℓ2 regularization, regular dropout, max-norm, skip connections or other non-sequential topologies (so recurrent neural networks won't self-normalize). However, in practice it works quite well with sequential CNNs. If you break self-normalization, SELU will not necessarily outperform other activation functions.\n",
    "\n",
    "\n",
    "\n",
    " During training, a neural network composed of a stack of dense layers using the SELU activation function will self-normalize: <br>\n",
    "\n",
    "The output of each layer will tend to preserve the same mean and variance during training, which solves the vanishing/exploding gradients problem. As a result, this activation function outperforms the other activation functions very significantly for such neural nets, so you should really try it out.\n",
    "$$selu(x)=\\lambda \\begin{cases} \\alpha\\exp(z) - \\alpha \\quad &if \\quad z<0 \\\\ x \\quad &if \\quad z \\geq 0 \\end{cases}$$\n",
    "We solve for $\\alpha$ and $\\lambda$ to find a fixed point that preserves mean and variance from layer to layer:\n",
    "$$g(\\mu, v, \\alpha, \\lambda) = (\\mu, v)$$\n",
    "Gradients neigher explode nor vanish. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pu-Vzxnf1soY"
   },
   "outputs": [],
   "source": [
    "from scipy.special import erfc\n",
    "\n",
    "# alpha and scale to self normalize with mean 0 and standard deviation 1\n",
    "# (see equation 14 in the paper):\n",
    "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EdUCXE9R1soZ"
   },
   "outputs": [],
   "source": [
    "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "colab_type": "code",
    "id": "AdZc0BYe1soa",
    "outputId": "2ce09316-6df4-4c88-855f-9ea554888e6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure selu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3gV1dn+8e9DwhkkCJiKILxWUakHxPxsUVtjoR4QRMWKCiq1CmKxYsETglJBUUSLVkGwWCoHBUVFDtpXbeOrxVqhUC0qeAAEj4AECOck6/fH2kjYCSE7mWRm731/rmsudvZMZp5Mhn1nZtasZc45REREoqZW2AWIiIiURQElIiKRpIASEZFIUkCJiEgkKaBERCSSFFAiIhJJCiiRAzCzKWY2rwa2k2tmzsya18C2+pnZ52ZWbGYjqnt7B6ilr5kVhFmDRJMCShJiZi3MbLyZrTKznWb2jZm9bma/KLFMXuyDNn56psQyzswuLmP9bWPzcsqYl2dmj1bjz7a/gLgR6BPwtlaZ2ZC4txcChwIbgtxWGdtuCjwGPAAcBoytzu3Fbbus3/tM4IiaqkGSR2bYBUjSmQ00AH4NfAIcApwBNItb7s/A0Lj3tld7ddXAObephrazC/i6BjbVBv9/f55z7qsa2F65nHPbSdJjQ6qXzqCkwswsC/gpcJtz7nXn3Grn3LvOubHOuWfiFt/mnPs6bqrWD3oz+6GZzTGzr81sq5n928y6xS1Tx8zuNbPVsTPAz8zst2bWFvh7bLF1sb/0p8S+5/tLfGbWP3bWmBm33hlmNqcidZhZHj4kHthzdhl7v9QZnJldZGbvx2pdY2Z3mJmVmL/KzIaZ2UQz22xma83s5nL2UV9gSezLz2Lba2tmI8zsv/HLlrz0tmcZM7vUzD41sy1m9mL8GaeZXVWi5m9K7MdVsUWejW13VVnbKbGfPzGzXbF/r42b72KXKZ+N7ePPzCzQs1wJnwJKElEQm843s3phF1OGRsDLwC+AE/Fne8+b2TEllvkLcCXwO+BY/JlgPrAG6Blb5kf4S203lrGNWUAW0GXPG2bWEOgBTKtgHRcBa4G7Y9s5tKwfxsxOBp4FngeOB24DbgcGxi16E/A+0BG4HxhjZp3KWif+cto5sdenxLa9Zj/LlqUt0Au4EDgLOAm4p0TN/YGJ+DPoE4CuwLLY7P8X+/fa2Hb3fL0PM7sQeBQYBxwHPAyMN7PucYveCczB7+OZwJNm1iaBn0WizjmnSVOFJ/yH+HfADuBt/P2LH8ctkwfsYm+g7ZmuL7GMAy4uY/1tY/NyypiXBzyaYL3/BIbFXh8VW/c5+1k2Nza/edz7U/CXw/Z8/QIwtcTXfYBNQL2K1BH7ehUwpLztA9OBv8UtMwJYG7eep+OW+bjktsqoJSe2nbZx6/1v3HJ9gYK4ZXYATUq8dwfwSYmv1wL3lbPtUr/3MrbzD+DJMn4Hb8WtZ3SJrzOBbUCfsP+PaApu0hmUJMQ5NxtoCXTHnyWcCvzTzOLvN80EOsRN06uzNjNraGZjzOwDM9sYu2yUAxweW+QkoJi9l/IqaxpwgZk1iH3dG3jOObejgnVU1LH4D+uS3gIOM7ODSrz3XtwyX+LvDVaH1W7fS7Xfb8vMDsE3uni9itvY38/dPu69739u51whsI7q+7klBGokIQmLfRC/GpvuNrM/ASPMbKzzN/oBNjnnPqnE6vd8+DUpY15WifllGYu/fDUEfxaxDXgKqBObb/v5vkTNAwqBHmb2Ov5y31kJ1FFRhj9TKEvJ93eXMS/RPz6LKb1/apexXHnbCmr/7lnvgd4L4ueWCNMvU4LwAf6PnSrfl3LObQTWAyeXfD92xnAksLycbz8deMo5N9s59x7+ctMPS8z/N/6YP3M/378nXDMOUONO4Dn8mVMvfMu7NxKoY8+2yt0Ofr+eHvfe6fhLfFsO8L2JWgdkl2yAgT/rrTDn3DfAF0DnchbbzYF/7g8p++f+IJF6JPnpDEoqzMya4W/aP4m/vLIFf+nqFuB159zmEos3MLMfxK1il3PuuxJftzWz+A/Bz4CHgNvM7Ev8fa5mwHB8cD1bTokrgAtjrel2A3dRIjSdcx+b2SzgT2Z2Iz6wWuHvxUwFVuP/Cj/PzOYC251z+3uAdBrwGvA/wAznXHFF64hZBfzUzKYBO51z68vYxoPAu+YfpJ2Bb1QwmNLN94OQBxwMDDX/vFouUOo5tQq4B/iDmX0DzMc/ktDZOfdgbP4qoLOZvYH/uTeWsY4H8C39FgP/iz8b7Y1vXCLpJOybYJqSZwLqAvcC7wIb8ZeuPsYHysEllsvDf9DHT/E3ucuauuH/wr4BH4IF+DOQZyhxU38/9bXBh8bW2PcMwV+OmxL3M4zB/6W/E/gUGFhi/nDgK/wlrymx96ZQopFE7D3Df9g64PhK1PET4D/4Rgcu9l4ucY008B/K7+PPuNbgGyVYifmrKN3YIo9yGpNQRiOJ2Pv98SG9Nba/b6R0I4lyG1LE3vs1/mxnz3NdT5aY1z12zOwGVpWzjuvwz9ntjv17bdz8shpblNoXmpJ7stgvVkREJFJ0D0pERCJJASUiIpGkgBIRkUhSQImISCSF3sy8efPmrm3btmGXUcrWrVtp2LBh2GUkHe23xCxfvpyioiLat4/vJEHKE9XjrKAAVqwA56BVK8jODruivaK6zwAWL1683jnXIv790AOqbdu2LFq0KOwySsnLyyM3NzfsMpKO9lticnNzyc/Pj+T/gSiL4nG2YgV06uTDaeBAeOQRsCD71qiiKO6zPcxsdVnv6xKfiEgVrVsHXbvCd99Bt24wbly0wilZKaBERKpg+3bo0QM+/RQ6doSnn4aMA3XmJBWigBIRqaTiYrjqKnj7bWjdGubNg0aNwq4qdSigREQqaehQePZZaNwY5s+HQ8scelIqK9CAMrNpZvZVbOjpFWZ2TZDrFxGJiieegPvv95fznnsOjj8+7IpST9BnUKPxHVAeBJwPjIoNWy0ikjL++lcYMMC/fvxxOOus8peXygk0oJxzy5wfKwf29k4dPw6OiEjSeu89+OUvoagIbr8drtF1omoT+HNQZjYe331+fWAJsKCMZfoB/QCys7PJy8sLuowqKygoiGRdUaf9lpj8/HyKioq0zxIU1nG2fn0drr++I1u21OPMM7+lS5cPSJZfXTL+36yW4TbMLAPohB/f5n7nXPzQzN/LyclxUXxIMcoPtUWZ9lti9jyou3Tp0rBLSSphHGcFBfCzn8GSJXDaafDaa1CvymNI15wo/980s8XOuZz496ulFZ9zrsg59xZ+tNIB1bENEZGaUlgIl17qw+nII+HFF5MrnJJVdTczz0T3oEQkiTkHN97om5E3awYLFkDz5mFXlR4CCygzO8TMLjWzRmaWYWZnA5cBfwtqGyIiNe0Pf4Dx46FOHX/mdNRRYVeUPoJsJOHwl/MexwffamCQc25OgNsQEakxL7wAQ4b413/5C5x+erj1pJvAAso5tw44I6j1iYiE6Z13oHdvf4nv3nv9PSipWerqSEQkzsqV0L277wj217+G224Lu6L0pIASESlh40Y/dMa6dfCLX8CECRo6IywKKBGRmF27oGdP+OgjOO443xFs7dphV5W+FFAiIvh7TddeC3//O/zgB75ZeZMmYVeV3hRQIiLA3XfDU09BgwZ+XKfDDw+7IlFAiUjamzoVRoyAWrXgmWfgZI3BEAkKKBFJa3l5vqUewLhxvvWeRIMCSkTS1ocfwoUXwu7dMGgQ3HBD2BVJSQooEUlL334L550H+fnQoweMHRt2RRJPASUiaWf7djj/fP9Abk4OTJ/uh26XaFFAiUhaKS6GK67wXRm1aQNz50LDhmFXJWVRQIlIWrn1Vpg92z/jNH++f+ZJokkBJSJpY8IEf68pM9OH1I9+FHZFUh4FlIikhQULYOBA//qJJ6Bz53DrkQNTQIlIylu6FHr18vefhg2Dvn3DrkgqQgElIilt7VrfnLygAC6/3HdpJMlBASUiKWvzZh9OX34JP/sZPPmkhs5IJgooEUlJhYX+st5770G7dn749rp1w65KEqGAEpGU45xvEPHKK9C8uW8gcfDBYVcliVJAiUjKGTsWJk70Z0wvvQQ//GHYFUllKKBEJKU8+yzccot/PXUqdOoUbj1SeQooEUkZb7/tuzECuP9++OUvw61HqkYBJSIp4dNPfQewO3dCv35w881hVyRVpYASkaT33XfQtSusXw/nnAOPPabm5KlAASUiSW3nTrjgAlixAk44AWbO9H3tSfJTQIlI0nIOrr4a3nwTWrb0vZMfdFDYVUlQFFAikrTuugtmzPDjOc2bB61ahV2RBEkBJSJJacoUGDkSatWCWbPgpJPCrkiCpoASkaSzeHEW117rXz/6qG8gIalHASUiSeWDD+Cuu46jsBAGD4YBA8KuSKqLAkpEksbXX/uzpa1bM+nZE8aMCbsiqU4KKBFJClu3QvfusHo1HHvsZqZO9fefJHUF9us1s7pmNtnMVpvZFjNbYmbnBrV+EUlfRUXQuzcsWgT/8z9wzz3vU79+2FVJdQvy749MYA1wBtAEGA7MMrO2AW5DRNLQkCEwZw5kZflnnZo23R12SVIDAgso59xW59wI59wq51yxc24esBI4OahtiEj6efRRGDcOatf2gw4ee2zYFUlNqbYOQcwsG2gHLCtjXj+gH0B2djZ5eXnVVUalFRQURLKuqNN+S0x+fj5FRUXaZ/uxcGEzhg8/DjBuvvlD4Bvy8nScVUYy7jNzzgW/UrPawMvAp865/uUtm5OT4xYtWhR4DVWVl5dHbm5u2GUkHe23xOTm5pKfn8/SpUvDLiVyFi+Gn/0Mtm2DESN8rxF76DhLXJT3mZktds7lxL8feBsYM6sFTAV2AQODXr+IpL7PP4du3Xw4XXkl3Hln2BVJGAK9xGdmBkwGsoGuzjndyRSRhGzaBOed5595OvNMeOIJDZ2RroK+BzUBOBbo4pzbHvC6RSTF7d7tR8H973/hmGNg9myoUyfsqiQsQT4H1QboD3QAvjazgtjUO6htiEjqcs53W/Tqq3DIIbBgATRtGnZVEqbAzqCcc6sBnYiLSKXcdx9Mngz16sFLL/kHciW9qaMQEQndM8/A0KH+XtP06fDjH4ddkUSBAkpEQvXWW9C3r389dixcdFGo5UiEKKBEJDQffww9esDOnXD99XDTTWFXJFGigBKRUKxf74fO+O47/+/DD6s5uexLASUiNW7HDrjgAvjkEz9U+8yZkFltHa9JslJAiUiNKi6GX/0K/vEPaNUK5s2DRo3CrkqiSAElIjVq2DDfaq9xYz90RsuWYVckUaWAEpEa86c/wejRkJEBzz4LJ5wQdkUSZQooEakRr74K113nX48fD2efHW49En0KKBGpdu+/Dxdf7Iduv/VW6Ncv7IokGSigRKRaffml751882a45BK4996wK5JkoYASkWpTUADdu8OaNdCpE0yZArX0qSMVpENFRKpFURFcfjn8+9/wwx/CnDlQv37YVUkyUUCJSOCcg0GDYO5cOPhgP3RGixZhVyXJRgElIoF7+GF49FE/2OCLL0K7dmFXJMlIASUigXrxRfjd7/zrP/8ZfvrTcOuR5KWAEpHAvPuuv+/kHIwa5V+LVJYCSkQCsWqVb7G3fTtcfbUfgFCkKhRQIlJl+fl+yIxvvoHOneHxxzV0hlSdAkpEqmTXLujZEz78ENq3h+eeg9q1w65KUoECSkQqzTno3x/+9jf4wQ98c/KsrLCrklShgBKRSrvnHt87RIMG/pmnNm3CrkhSiQJKRCpl+nQYPtzfa5oxA3Jywq5IUo0CSkQS9n//51vqAfzhD9CjR7j1SGpSQIlIQpYvhwsu8I0jfvtbuPHGsCuSVKWAEpEKW7fONyffuBHOPx8eeijsiiSVKaBEpEK2b/eh9NlncPLJ/r5TRkbYVUkqU0CJyAEVF8OVV8I//wmHH+5b7DVsGHZVkuoUUCJyQLff7h/APeggmD8fDj007IokHSigRKRcEyfCmDGQmQmzZ8Nxx4VdkaQLBZSI7Ncrr8BvfuNfT5wIXbqEW4+kFwWUiJTpP/+BX/7SD91+xx17n3sSqSkKKBEp5Ysv4LzzoKAALrsMRo4MuyJJR4EGlJkNNLNFZrbTzKYEuW4RqRlbtkC3bj6kTj/dj4qroTMkDJkBr+9LYBRwNlA/4HWLSDUrLIRevWDpUjjqKD98e926YVcl6SrQgHLOPQ9gZjlAqyDXLSLVyznfddHLL0OzZn7ojGbNwq5K0lnQZ1AVYmb9gH4A2dnZ5OXlhVFGuQoKCiJZV9RpvyUmPz+foqKiSOyzWbNaMWHCkdSuXcyIEUtZu3Yza9eGXVXZdJwlLhn3WSgB5ZybBEwCyMnJcbm5uWGUUa68vDyiWFfUab8lJisri/z8/ND32ezZfph2gGnTanHJJR1DredAdJwlLhn3mVrxiaS5f/4T+vTxl/hGj4ZLLgm7IhFPASWSxj77zHcAu2MHXHst3Hpr2BWJ7BXoJT4zy4ytMwPIMLN6QKFzrjDI7YhI1W3c6J91WrcOzjoLHntMzcklWoI+gxoGbAduA/rEXg8LeBsiUkU7d8JFF8FHH8Hxx8Ozz0Lt2mFXJbKvoJuZjwBGBLlOEQmWc/5yXl6e75V8/nzfS7lI1OgelEia+f3vYepUP57TvHnQunXYFYmUTQElkkaeesoHVK1aMHMmdIx2a3JJcwookTTx97/DNdf414884htIiESZAkokDXz4IVx4IezeDTfdtHeMJ5EoU0CJpLhvvoGuXWHTJh9SDzwQdkUiFaOAEklh27b5B3FXrYJTToFp0yAjI+yqRCpGASWSooqKfBdG//oXtG0LL70EDRqEXZVIxSmgRFLULbfACy9Akyb+Wafs7LArEkmMAkokBY0fDw895HuHeP55aN8+7IpEEqeAEkkx8+fDDTf41088AT//ebj1iFSWAkokhSxZ4odsLy6GO++Eq64KuyKRylNAiaSINWv8w7dbt/rGESNGhF2RSNUooERSwObNPpy++grOOAP+9CcNnSHJTwElkuR27/aj4L7/Phx9tG+5V7du2FWJVJ0CSiSJOee7LfrrX6FFC1iwAJo2DbsqkWAooESS2JgxvqVevXr+Qdwjjgi7IpHgKKBEktSsWXDbbf5e07Rp8JOfhF2RSLAUUCJJaOFCuPJK/3rMGOjZM9x6RKqDAkokyXzyCfToATt3wnXXweDBYVckUj0UUCJJZMMGP3TG+vVw7rnwxz+qObmkLgWUSJLYudOP5/Txx3DiiX7I9szMsKsSqT4KKJEk4BxcfTW8+SYcdpjvb69x47CrEqleCiiRJHDnnTBjBjRq5MPpsMPCrkik+imgRCLuySdh1Cg/Eu6sWf7ynkg6UECJRNhrr0H//v71Y4/5hhEi6UIBJRJRy5b555sKC+Hmm/cGlUi6UECJRNDXX/vm5Js3w8UXw333hV2RSM1TQIlEzNat0K0bfP65777oqaeglv6nShrSYS8SIUVFcPnlsHix7/h1zhyoXz/sqkTCoYASiZDBg32v5E2b+qEzDjkk7IpEwqOAEomIRx6Bhx+G2rX9oINHHx12RSLhUkCJRMBLL8GgQf71k0/6YdtF0l2gAWVmB5vZC2a21cxWm9nlQa5fJBVt25bBZZf57ozuvhv69Am7IpFoCLqryceAXUA20AGYb2b/cc4tC3g7Iilh505YubIhhYXQty8MGxZ2RSLRYc65YFZk1hDYCBznnFsRe28q8IVz7rb9fV/jxo3dySefHEgNQcrPzycrKyvsMpKO9lti/vGPpRQWQlZWB044QUNnVJSOs8RFeZ+98cYbi51zOfHvB3kG1Q4o2hNOMf8BSl1NN7N+QD+A2rVrk5+fH2AZwSgqKopkXVGn/VZxGzfWobDQv27ZcjObNhWHW1AS0XGWuGTcZ0EGVCNgU9x7m4BSgwI45yYBkwBycnLcokWLAiwjGHl5eeTm5oZdRtLRfquY9evhmGMAcmnVahvLlv0r7JKSio6zxEV5n9l+Lh0E2UiiADgo7r2DgC0BbkMkJYwc6UfHzcqCZs12hV2OSCQFGVArgEwzO6rEeycCaiAhUsJnn8GECf5+05FHhl2NSHQFFlDOua3A88DdZtbQzE4DegBTg9qGSCq44w7YvRuuuAIaNgy7GpHoCvpB3euB+sC3wNPAADUxF9nrX/+CZ56BunX9ZT4R2b9An4Nyzn0HXBDkOkVSRXEx/Pa3/vWgQXD44eHWIxJ16upIpIZMnQrvvAOHHuov84lI+RRQIjVg82a49Vb/eswYaFzq4QsRiaeAEqkBI0fCN99Ap07Qu3fY1YgkBwWUSDX76CMYN843K//jH9WdkUhFKaBEqpFzcNNNUFgI11wDEex2UiSyFFAi1WjWLHjlFWjSBO65J+xqRJKLAkqkmqxfDzfc4F8/8AC0aBFuPSLJRgElUk1uugnWrYMzz/SX90QkMQookWrw8sswbRrUqweTJqlhhEhlKKBEArZlC/Tv71+PHKkOYUUqSwElErDbb4c1a3yLvUGDwq5GJHkpoEQC9Mor8NhjkJkJkyf7f0WkchRQIgH59lvo29e//v3v4cQTQy1HJOkpoEQC4Bz8+te+O6Mzztjb756IVJ4CSiQA48fDvHl+CPepUyEjI+yKRJKfAkqkipYtgyFD/OsnnoDWrcOtRyRVKKBEqqCgAHr1gh074Oqr4eKLw65IJHUooEQqac99p2XL4Jhj4OGHw65IJLUooEQqaexY3xls48bw4ovQqFHYFYmkFgWUSCW89hrcdpt//dRTcPTR4dYjkooUUCIJWr0aLr0UiovhjjvgggvCrkgkNSmgRBKweTOcfz5s2ADnnOMfyBWR6qGAEqmg3bt9K7333oN27WD6dD3vJFKdFFAiFeCc76H81Vf9wIMvvwwHHxx2VSKpTQElUgEjR8Kf/wz16/seI444IuyKRFKfAkrkACZPhrvuglq14Jln4JRTwq5IJD0ooETKMWMGXHutf/3II76BhIjUDAWUyH489xxceaW//3TPPfCb34RdkUh6UUCJlGHuXLjsMigqguHDYejQsCsSST8KKJE48+f75uSFhb6Xcj3rJBIOBZRICU8/7XuG2LULBg6EMWPALOyqRNKTAkok5vHHoXdvf+Z0yy2+UYTCSSQ8CigR4L77YMAA3yBi9Gi4/36Fk0jYAgkoMxtoZovMbKeZTQlinSI1obAQbrgBbr/dB9L48Xt7KReRcGUGtJ4vgVHA2UD9gNYpUq02bfKj4f71r1CnDvzlL76XchGJhkACyjn3PICZ5QCtglinSHVauRK6dYMPPvB9673wApx2WthViUhJQZ1BJcTM+gH9ALKzs8nLywujjHIVFBREsq6oS4b9tnhxFqNGtSc/vw5t2mxl9Oj32b17B2GUnZ+fT1FRUeT3WdQkw3EWNcm4z0IJKOfcJGASQE5OjsvNzQ2jjHLl5eURxbqiLsr7rbgY7r0X7rzTN4Y4+2yYObMhTZr8JLSasrKyyM/Pj+w+i6ooH2dRlYz77ICNJMwsz8zcfqa3aqJIkapavx7OO8/3CgE+pObPhyZNwq1LRPbvgGdQzrncGqhDpNr87W9w1VWwdq0fw2n6dD8arohEW1DNzDPNrB6QAWSYWT0zC+Xyocge27fDTTdB584+nH78Y1iyROEkkiyCelB3GLAduA3oE3s9LKB1iyRs8WI4+WQYN84Pyz5iBLz5Jhx+eNiViUhFBdXMfAQwIoh1iVRFQYEPo3HjfE/kxxwDU6dCTk7YlYlIotTVkaSMuXOhfXt48EHfSm/QIPj3vxVOIslK94kk6X3yiR8WY84c/3XHjjBpkr/EJyLJS2dQkrQ2boTf/c6fNc2ZA40a+Ut777yjcBJJBTqDkqSzfTtMnAijRsGGDb6T11/9yn/dsmXY1YlIUBRQkjR27oTJk+Gee+DLL/17Z5wBDz3kL+uJSGpRQEnkbdsGU6b4MZo+/9y/16ED3H237/BV4zaJpCYFlETWhg1+fKZHHvFdFQH86Efw+9/DhRdCLd1BFUlpCiiJnCVL/PDr06b5syfwTcVvvdUHU0ZGuPWJSM1QQEkkbNsGs2b5YHrnnb3vn3MO3HIL5ObqUp5IulFASWic8w/STp3qR7PNz/fvZ2X5zl3794djjw23RhEJjwJKatyKFfD00zBjhn+9xymnwHXX+WHYGzQIrz4RiQYFlNSIjz/2D9POnAmLFu19/5BDfCBddZUerhWRfSmgpFoUFsLChb5/vLlzYfnyvfMaN4aLLoLLL4ef/xwydRSKSBn00SCBWbkS5s8/lIkT4X//F777bu+8rCzo2tW3wjvvPKhfP7w6RSQ5KKCk0tas8WMsvf66H7V21SqAo7+ff9RR0L27n047DWrXDqtSEUlGCiipkJ07/fNJb7/tp4UL4Ysv9l2maVM47rh19OrVgi5d4Oijy16XiEhFKKCklG3b4L33fCDtmd57D3bt2ne5Jk2gUyd/H6lzZzjxRHjzzWXk5uaGUreIpBYFVBrbvt03XvjwQz998IGfli+H4uLSy7dv7wNpz3TMMepuSESqjwIqxe3Y4e8NrVwJn33mp48+8oG0apV/WDZeRgYcfzycdJKfOnb0Z0dNmtR09SKSzhRQSay42Hei+sUX+06ff743jOLvE5WUkeEbMhx77L5T+/ZqZSci4VNARYxzvsufdev2ndavh2+/3TeIvvwSdu8uf30ZGXD44XDEEXunPaF05JFQp07N/FwiIolSQFWD4mLYssUHTX4+bNq093VZ08aNfmiJPUFUWFjxbTVtCocdtu/UuvXeMGrdWg/CikhySpuPLud8K7QdO3yT6R079k5lfb1kSTaffOK/3roVCgoq/u/27VWrtXFjaNGi7Klly71B1LKl+qwTkdQVekB99RUMH+7PGnbv3v+/5c3b3zJ7AmlP6CSmat1oN2zoz26ysg48NWkCzZv7qUULqFu3SpsWEUkJ5spqxlWTBVhjB/G9hF4CXA9sA7qW8V19Y9N64OIy5g8AegFrgCtKbMs3i27YcDBNmnSnVq3lrFvXn1q12Gdq334YdeseT6NGX/Huu4sjaAwAAAYtSURBVIPIyPDvZ2T46dJL7+Wkk05l9eqFPPXU0FLzH354HB07duC1115j1KhRpaqbOHEiRx99NHPnzuXBBx8sNX/q1Km0bt2amTNnMmHChFLzn3vuOZo3b86UKVOYMmVKqfkLFiygQYMGjB8/nlmzZpWan5eXB8DYsWOZN2/ePvPq16/Pyy+/DMDIkSN5/fXX95nfrFkzZs+eDcDtt9/O22+/vc/82rVr8+qrrwIwaNAgli5dus/8du3aMWnSJAD69evHipLdmQMdOnRg3LhxAPTp04e1a9fuM79Tp06MHj0agJ49e7Jhw4Z95nfu3Jnhw4cDcO6557I97nS2W7duDBkyBKDM57UuueQSrr/+erZt20bXrqWPvb59+9K3b1/Wr1/PxReXPvYGDBhAr169WLNmDVdccUWp+YMHD6Z79+4sX76c/v37s3TpUgoLC8nJyQFg2LBhdOnShaVLlzJo0KBS33/vvfdy6qmnsnDhQoYOHVpq/rhx4+jQIfWPvd69e/NFXAugVq1aMW3aNEDHXlnH3llnncXQoUO/P/bihXnsvfHGG4udcznx3xP6GVSdOv5SldneqWNH/+BncbEf7rvkPDM46yzfn1tBAYwYUXp+nz5wwQX+ns6NN+4Nnj0GD/bd7yxf7sccijdsGGRmfkhWVhZl/J445xw49VTfm8KLL5aer2eDRESqLvQzqJycHLeo5PgLEZGXl6ceESpB+y0xubm55Ofnl/prX8qn4yxxUd5nZlbmGZT+1hcRkUhSQImISCQpoEREJJIUUCIiEkkKKBERiaQqB5SZ1TWzyWa22sy2mNkSMzs3iOJERCR9BXEGlYl/IvYMoAkwHJhlZm0DWLeIiKSpKj+o65zbCowo8dY8M1uJ7x5iVVXXLyIi6SnwniTMLBtoBywrZ5l+QD+A7Ozs77s/iZKCgoJI1hV12m+Jyc/Pp6ioSPssQTrOEpeM+yzQniTMrDbwMvCpc66MToRKU08SqUX7LTHqSaJydJwlLsr7rNI9SZhZnpm5/UxvlViuFjAV2AUMDLR6ERFJOwe8xOecyz3QMmZmwGQgG+jqnDvAOK8iIiLlC+oe1AT8AEpdnHNVHK5PREQkmOeg2gD9gQ7A12ZWEJt6V7k6ERFJW0E0M18NWAC1iIiIfE9dHYmISCQpoEREJJJCH1HXzNYBq0MtomzNgfVhF5GEtN8Sp32WOO2zxEV5n7VxzrWIfzP0gIoqM1tU1oNjUj7tt8RpnyVO+yxxybjPdIlPREQiSQElIiKRpIDav0lhF5CktN8Sp32WOO2zxCXdPtM9KBERiSSdQYmISCQpoEREJJIUUCIiEkkKqAoys6PMbIeZTQu7ligzs7pmNtnMVpvZFjNbYmbnhl1XFJnZwWb2gpltje2vy8OuKcp0bFVNMn6GKaAq7jHg3bCLSAKZwBrgDKAJMByYZWZtQ6wpqh7DD/CZDfQGJpjZj8ItKdJ0bFVN0n2GKaAqwMwuBfKB18OuJeqcc1udcyOcc6ucc8XOuXnASuDksGuLEjNrCPQEhjvnCpxzbwEvAVeEW1l06diqvGT9DFNAHYCZHQTcDQwOu5ZkZGbZQDtgWdi1REw7oMg5t6LEe/8BdAZVQTq2KiaZP8MUUAc2EpjsnFsTdiHJxsxqA9OBvzjnPgq7nohpBGyKe28T0DiEWpKOjq2EJO1nWFoHlJnlmZnbz/SWmXUAugB/CLvWqDjQPiuxXC1gKv4ey8DQCo6uAuCguPcOAraEUEtS0bFVccn+GVblEXWTmXMut7z5ZjYIaAt8bmbg/+rNMLP2zrmO1V5gBB1onwGY31mT8Tf/uzrndld3XUloBZBpZkc55z6OvXciulxVLh1bCcsliT/D1NVROcysAfv+lTsE/8se4JxbF0pRScDMHgc6AF2ccwVh1xNVZvYM4IBr8PtrAXCqc04htR86thKT7J9haX0GdSDOuW3Atj1fm1kBsCMZfrFhMbM2QH9gJ/B17K82gP7OuemhFRZN1wNPAt8CG/AfGgqn/dCxlbhk/wzTGZSIiERSWjeSEBGR6FJAiYhIJCmgREQkkhRQIiISSQooERGJJAWUiIhEkgJKREQiSQElIiKR9P8BcCYlA0V/62oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"selu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A75u08Xm1sob"
   },
   "source": [
    "By default, the SELU hyperparameters (`scale` and `alpha`) are tuned in such a way that the mean output of each neuron remains close to 0, and the standard deviation remains close to 1.\n",
    "Requirements: <br>\n",
    "- The input features must be standardized (mean 0 and standard deviation 1).\n",
    "- Every hidden layer’s weights must be initialized with LeCun normal initialization.\n",
    "In Keras, this means setting kernel_initializer=\"lecun_normal\".\n",
    "- The network’s architecture must be sequential. It will not work in \n",
    "in nonsequential architectures, such as recurrent networks, networks with skip connections (i.e., connections that skip layers, such\n",
    "as in Wide & Deep nets). \n",
    "-SELU will not necessarily outperform other activation functions.\n",
    "\n",
    "Using this activation function, even a 1,000 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "18i6E-w21sob",
    "outputId": "44029c9c-973e-4bfd-e875-1f0bb6e04183"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: mean -0.00, std deviation 1.00\n",
      "Layer 100: mean 0.02, std deviation 0.96\n",
      "Layer 200: mean 0.01, std deviation 0.90\n",
      "Layer 300: mean -0.02, std deviation 0.92\n",
      "Layer 400: mean 0.05, std deviation 0.89\n",
      "Layer 500: mean 0.01, std deviation 0.93\n",
      "Layer 600: mean 0.02, std deviation 0.92\n",
      "Layer 700: mean -0.02, std deviation 0.90\n",
      "Layer 800: mean 0.05, std deviation 0.83\n",
      "Layer 900: mean 0.02, std deviation 1.00\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100)) # standardized inputs\n",
    "for layer in range(1000):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1 / 100)) # LeCun initialization\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=0).mean()\n",
    "    stds = np.std(Z, axis=0).mean()\n",
    "    if layer % 100 == 0:\n",
    "        print(\"Layer {}: mean {:.2f}, std deviation {:.2f}\".format(layer, means, stds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqV2lp6w1sod"
   },
   "source": [
    "Using SELU is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fsEs_ZKZ1sod",
    "outputId": "f42e2928-f8b9-4fb9-ef99-c372ddccee27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x7fecf038f220>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"selu\",\n",
    "                   kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4qeNSbec1soe"
   },
   "source": [
    "Let's create a neural net for Fashion MNIST with 100 hidden layers, using the SELU activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xCc2ZRXk1soe"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHUqrYSq1sof"
   },
   "outputs": [],
   "source": [
    "# constiruct 100 layer model to guarntee overfitting\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"selu\",\n",
    "                             kernel_initializer=\"lecun_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\",\n",
    "                                 kernel_initializer=\"lecun_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UXAa5vuP1sog"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gCxb6PrM1soh"
   },
   "source": [
    "Now let's train it. Do not forget to scale the inputs to mean 0 and standard deviation 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UrQg62vd1soh"
   },
   "outputs": [],
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "KY31D0YM1soi",
    "outputId": "b020ad95-2d6c-4842-e3f1-188fa5164b45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 1.0538 - accuracy: 0.5901 - val_loss: 0.7711 - val_accuracy: 0.6858\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.7687 - accuracy: 0.7254 - val_loss: 0.7534 - val_accuracy: 0.7384\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.6835 - accuracy: 0.7554 - val_loss: 0.5943 - val_accuracy: 0.7834\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.5744 - accuracy: 0.7911 - val_loss: 0.5434 - val_accuracy: 0.8066\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.5406 - accuracy: 0.8114 - val_loss: 0.4907 - val_accuracy: 0.8218\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "69lgAabb1soj"
   },
   "source": [
    "Did not work. \n",
    "Now look at what happens if we try to use the ReLU activation function instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "23Fi_Wp51sok"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V7PPLtoV1sok"
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "53LylVZN1sol"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "vGxIJR1O1som",
    "outputId": "0360ea8f-70d5-4fad-a1c2-d85ef8011d49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 1.7806 - accuracy: 0.2781 - val_loss: 1.5971 - val_accuracy: 0.3048\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 1.1593 - accuracy: 0.5055 - val_loss: 0.9156 - val_accuracy: 0.6372\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.8950 - accuracy: 0.6358 - val_loss: 0.8928 - val_accuracy: 0.6246\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.7905 - accuracy: 0.6851 - val_loss: 0.6914 - val_accuracy: 0.7396\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.7051 - accuracy: 0.7287 - val_loss: 0.6638 - val_accuracy: 0.7380\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1caTL16j1son"
   },
   "source": [
    "How to choose activation function:\n",
    "\n",
    "1. ReLU is a good fast default for first estimation. Many packages have ReLU default and are optimized for it's use. Leaky ReLU is also quite fast. \n",
    "2. In general quality is ranked by SELU>ELU>Leaky ReLU, > ReLU, tanh > logistics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hnyFUTDYEqga"
   },
   "source": [
    "# Batch Normalization\n",
    "\n",
    "He intialization and ELU can reduce the danger of vanishing/exploding gradients at the beginning of training the problem may arise later in the training process. \n",
    "\n",
    "Batch Normalization zero-centers and normilizes the data before each layer. So each layer has two new parameters for centering (mean) and scaling (standard deviation). \n",
    "\n",
    "In many case if you add BN before the first layer you may not need to scale your data. \n",
    "\n",
    "BN algorithm:\n",
    "\n",
    "1. $\\mu_B = \\frac{1}{m_B}\\sum_{i=1}^{m_B}x^{(i)}$\n",
    "2. $\\sigma_B = \\frac{1}{m_B}\\sum_{i=1}^{m_B}(x^{(i)}- \\mu_B)^2$\n",
    "3. $\\hat{x}^{(i)} = \\frac{x^{(i)} - \\mu_{B}}{\\sqrt{\\sigma_{B}^2 + \\epsilon}}$\n",
    "4. $z^{(i)} = \\gamma \\otimes \\hat{x}^{(i)} + \\beta$\n",
    "\n",
    "\n",
    "$\\mu_B$, $\\sigma_B$ is the vector of output means and standard deviations evaluated for each batch. $m_B$ is the number of instances in a batch and $\\hat{x}^{(i)}$ is the vectors of normilized inputs for instance $i$. \n",
    "\n",
    "* $\\gamma$ s the output scale parameter vector for the layer\n",
    "* $\\otimes$ represents element-wise multiplication\n",
    "* $\\beta$ is the output shift (offset) parameter vector for the layer (it contains one offset parameter per input). Each input is offset by its corresponding shift parameter.\n",
    "* $\\epsilon$ is a tiny number that avoids division by zero (typically 10–5). This is called a smoothing term.\n",
    "* $z^{(i)}$ is the output of the BN operation. It is a rescaled and shifted version of the\n",
    "inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WdjGLuAn6wPa"
   },
   "source": [
    "The BatchNormalization usese moving average to normilized data inside network.\n",
    "\n",
    "Four parameter vectors are learned in each batch-normalized layer: γ (the output scale vector) and β (the output offset vector) are learned through regular backpropagation, and μ (the final input mean vector) and σ (the final input standard deviation vector) are estimated using an exponential moving average.\n",
    "\n",
    "Note that μ and σ are estimated during training, but they are used only after training (to replace the batch input means and standard deviations.\n",
    "\n",
    "Batch normalization significantly improve prediction in ImageNet classification problems (Ioffe and Szegedy) it also eliminates the need for other regularization (LASSO, RIDGE, etc). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uxy87AcS8Ybr"
   },
   "source": [
    "Batch Normilization slows down the model estimation, although it makes the convergence faster, often reducing the total time for model estimation. \n",
    "\n",
    "If a regular NN model estimates $XW + b$ in each layer, the BN+NN model estimates $\\gamma \\otimes (XW +b - \\mu)/\\sigma + \\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z48v61Zh8tOj"
   },
   "source": [
    "In Keras you can just add `BatchNormalization` layer before and after each hidden layer. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ab8XPS4IEqga"
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t8EN8quU9YHg"
   },
   "source": [
    "In a small model with few layers `BatchNormalization` may not have a large impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "sgEhTEfrEqgb",
    "outputId": "6ccf6b23-d3ea-4ad9-8a0f-fe74725f62e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_209 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_210 (Dense)            (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_211 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h_ZDZG419kaF"
   },
   "source": [
    "First layer of BN adds 3,136 parameters. $\\mu$ and $\\sigma$ are just moving averages and they are not affected by back-propagations, so Keras calls them non-trainable. In total BN adds 4,736 parameters out which 2,368 are \"non-trainable\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Ro8lYeWBEqgc",
    "outputId": "b737f2eb-d6e5-4601-efd6-3199557b989c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn1 = model.layers[1]\n",
    "[(var.name, var.trainable) for var in bn1.variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "npeFOm97Eqge"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "colab_type": "code",
    "id": "8AYkZXITEqge",
    "outputId": "59981225-cd9e-4923-dd0c-1de3e3df6db2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.8750 - accuracy: 0.7123 - val_loss: 0.5526 - val_accuracy: 0.8230\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5753 - accuracy: 0.8030 - val_loss: 0.4725 - val_accuracy: 0.8468\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.5189 - accuracy: 0.8203 - val_loss: 0.4375 - val_accuracy: 0.8558\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.4827 - accuracy: 0.8322 - val_loss: 0.4153 - val_accuracy: 0.8596\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.4565 - accuracy: 0.8407 - val_loss: 0.3997 - val_accuracy: 0.8640\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.4398 - accuracy: 0.8475 - val_loss: 0.3867 - val_accuracy: 0.8694\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.4241 - accuracy: 0.8513 - val_loss: 0.3763 - val_accuracy: 0.8710\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4144 - accuracy: 0.8539 - val_loss: 0.3711 - val_accuracy: 0.8740\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.4023 - accuracy: 0.8582 - val_loss: 0.3631 - val_accuracy: 0.8752\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 0.3914 - accuracy: 0.8624 - val_loss: 0.3573 - val_accuracy: 0.8750\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4WQ4HUwIAVWI"
   },
   "source": [
    "In our example, the save time was negligible. Though we reached a bit higher validation accuracy the convergence speeed decreased (accuracy growth was slower).\n",
    "\n",
    "Batch normalization has a lot of parameters to optimize, see pages 343-344 for more details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l6KKVUWSEqgj"
   },
   "source": [
    "## Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bqMGhTFTeYO-"
   },
   "source": [
    "It is generally not a good idea to train a very large DNN from scratch: instead, you should always try to find an existing neural network that accomplishes a similar task to the one you are trying to tackle, then just reuse the lower layers of this network: this is called transfer learning.\n",
    "\n",
    "It will not only speed up training considerably, but will also require much less training data.\n",
    "\n",
    "For example, suppose that you have access to a DNN that was trained to classify pictures into 100 different categories, including animals, plants, vehicles, and everyday objects. You now want to train a DNN to classify specific types of vehicles. These tasks are very similar, so you should try to reuse parts of the first network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "alqz4s20efqW"
   },
   "source": [
    "![Neuron inner working g](images/Figure_11_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2wcP6bTsfhGe"
   },
   "source": [
    "Recycling lower levels is the most useful because the break picture into segements that can be undetstood by a machine learning, and coarse classification. Upper level are more task-specific and they need to be estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-45KUQu4Eqgj"
   },
   "source": [
    "### Reusing a Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sYQitmWXEqgj"
   },
   "source": [
    "Let's split the fashion MNIST training set in two:\n",
    "* `X_train_A`: all images of all items except for sandals and shirts (classes 5 and 6).\n",
    "* `X_train_B`: a much smaller training set of just the first 200 images of sandals or shirts.\n",
    "\n",
    "The validation set and the test set are also split this way, but without restricting the number of images.\n",
    "\n",
    "We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (sneakers, ankle boots, coats, t-shirts, etc.) are somewhat similar to classes in set B (sandals and shirts). However, since we are using `Dense` layers, only patterns that occur at the same location can be reused (in contrast, convolutional layers will transfer much better, since learned patterns can be detected anywhere on the image, as we will see in the CNN chapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SY0Uu7pjEqgk"
   },
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "# split data into training, validation and testing\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "# get small training data to simulate low-resource model\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gbqKbMSSEqgk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43986, 28, 28)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PzFVJWZeEqgl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 28, 28)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small trianing\n",
    "X_train_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IWf1cqfmEqgm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 5, 7, 7, 7, 4, 4, 3, 4, 0, 1, 6, 3, 4, 3, 2, 6, 5, 3, 4, 5,\n",
       "       1, 3, 4, 2, 0, 6, 7, 1], dtype=uint8)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Full training\n",
    "y_train_A[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y-lyTsbQEqgm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sandals or shirts?\n",
    "y_train_B[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5EpRphWSEqgn"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OgB5qW3wEqgo"
   },
   "outputs": [],
   "source": [
    "# Main model specification 5 layers, like in the picture.\n",
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cU70MJ1ZEqgo"
   },
   "outputs": [],
   "source": [
    "# model construciton\n",
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "arljodw5Eqgp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.5926 - accuracy: 0.8104 - val_loss: 0.3896 - val_accuracy: 0.8662\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 1s 983us/step - loss: 0.3523 - accuracy: 0.8786 - val_loss: 0.3288 - val_accuracy: 0.8827\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 1s 1ms/step - loss: 0.3171 - accuracy: 0.8895 - val_loss: 0.3013 - val_accuracy: 0.8991\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 1s 972us/step - loss: 0.2973 - accuracy: 0.8975 - val_loss: 0.2896 - val_accuracy: 0.9021\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 1s 1ms/step - loss: 0.2835 - accuracy: 0.9020 - val_loss: 0.2773 - val_accuracy: 0.9061\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 1s 1ms/step - loss: 0.2730 - accuracy: 0.9061 - val_loss: 0.2735 - val_accuracy: 0.9066\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 1s 1ms/step - loss: 0.2642 - accuracy: 0.9093 - val_loss: 0.2721 - val_accuracy: 0.9081\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 1s 1ms/step - loss: 0.2573 - accuracy: 0.9128 - val_loss: 0.2589 - val_accuracy: 0.9141\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 1s 1ms/step - loss: 0.2519 - accuracy: 0.9136 - val_loss: 0.2562 - val_accuracy: 0.9136\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2469 - accuracy: 0.9152 - val_loss: 0.2544 - val_accuracy: 0.9160\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2423 - accuracy: 0.9176 - val_loss: 0.2495 - val_accuracy: 0.9153\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2383 - accuracy: 0.9185 - val_loss: 0.2515 - val_accuracy: 0.9126\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2351 - accuracy: 0.9198 - val_loss: 0.2446 - val_accuracy: 0.9160\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2316 - accuracy: 0.9213 - val_loss: 0.2415 - val_accuracy: 0.9178\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2287 - accuracy: 0.9212 - val_loss: 0.2447 - val_accuracy: 0.9195\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2254 - accuracy: 0.9226 - val_loss: 0.2384 - val_accuracy: 0.9198\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2231 - accuracy: 0.9234 - val_loss: 0.2412 - val_accuracy: 0.9175\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2201 - accuracy: 0.9245 - val_loss: 0.2429 - val_accuracy: 0.9158\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2178 - accuracy: 0.9255 - val_loss: 0.2330 - val_accuracy: 0.9205\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2156 - accuracy: 0.9261 - val_loss: 0.2333 - val_accuracy: 0.9200\n"
     ]
    }
   ],
   "source": [
    "# train Task A\n",
    "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
    "                    validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bRSVt4FZEqgq"
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "model_A.save(\"my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w0JrJsZKEqgr"
   },
   "outputs": [],
   "source": [
    "# Model B has the same architecture as A\n",
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6a-cJuo0Eqgr"
   },
   "outputs": [],
   "source": [
    "# Compile model B\n",
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sd76XflUEqgs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.9573 - accuracy: 0.4650 - val_loss: 0.6314 - val_accuracy: 0.6004\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.5692 - accuracy: 0.7450 - val_loss: 0.4784 - val_accuracy: 0.8529\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.4503 - accuracy: 0.8650 - val_loss: 0.4102 - val_accuracy: 0.8945\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3879 - accuracy: 0.8950 - val_loss: 0.3647 - val_accuracy: 0.9178\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3435 - accuracy: 0.9250 - val_loss: 0.3300 - val_accuracy: 0.9320\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3081 - accuracy: 0.9300 - val_loss: 0.3019 - val_accuracy: 0.9402\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2800 - accuracy: 0.9350 - val_loss: 0.2804 - val_accuracy: 0.9422\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2564 - accuracy: 0.9450 - val_loss: 0.2606 - val_accuracy: 0.9473\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2362 - accuracy: 0.9550 - val_loss: 0.2428 - val_accuracy: 0.9523\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2188 - accuracy: 0.9600 - val_loss: 0.2281 - val_accuracy: 0.9544\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2036 - accuracy: 0.9700 - val_loss: 0.2150 - val_accuracy: 0.9584\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1898 - accuracy: 0.9700 - val_loss: 0.2036 - val_accuracy: 0.9584\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1773 - accuracy: 0.9750 - val_loss: 0.1931 - val_accuracy: 0.9615\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1668 - accuracy: 0.9800 - val_loss: 0.1838 - val_accuracy: 0.9635\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1570 - accuracy: 0.9900 - val_loss: 0.1746 - val_accuracy: 0.9686\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1481 - accuracy: 0.9900 - val_loss: 0.1674 - val_accuracy: 0.9686\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1406 - accuracy: 0.9900 - val_loss: 0.1604 - val_accuracy: 0.9706\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1334 - accuracy: 0.9900 - val_loss: 0.1539 - val_accuracy: 0.9706\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1268 - accuracy: 0.9900 - val_loss: 0.1482 - val_accuracy: 0.9716\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1208 - accuracy: 0.9900 - val_loss: 0.1431 - val_accuracy: 0.9716\n"
     ]
    }
   ],
   "source": [
    "# Estimate Model B without recycling\n",
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BlasHPvGEqgt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_212 (Dense)            (None, 300)               235200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_213 (Dense)            (None, 100)               30000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_214 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Final validation accuracy 0.9767\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pc0ACey_Eqgt"
   },
   "outputs": [],
   "source": [
    "# Load model A\n",
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "# Recycle all layers except the last one \n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "# Add 1 layer to the recycled layer\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zNig2SL_Eqgu"
   },
   "outputs": [],
   "source": [
    "# clone model A’s architecture with\n",
    "\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "#copy its weights (since clone_model() does not clone the weights)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqmMsWEyEqgv"
   },
   "outputs": [],
   "source": [
    "# We will freeze the weight for the first few epochs in the recycled layers so the top layer can adjust to recycled weights and data. \n",
    "#If don't freeze them the gradients may become unstable.\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "  # stop training the recycled layer\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                     metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UTxjovPGEqgw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.5802 - accuracy: 0.6500 - val_loss: 0.5843 - val_accuracy: 0.6329\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5437 - accuracy: 0.6800 - val_loss: 0.5467 - val_accuracy: 0.6805\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5068 - accuracy: 0.7250 - val_loss: 0.5146 - val_accuracy: 0.7089\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.4752 - accuracy: 0.7500 - val_loss: 0.4859 - val_accuracy: 0.7323\n",
      "Epoch 1/16\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 0.3965 - accuracy: 0.8150 - val_loss: 0.3460 - val_accuracy: 0.8661\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2800 - accuracy: 0.9350 - val_loss: 0.2603 - val_accuracy: 0.9310\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2085 - accuracy: 0.9650 - val_loss: 0.2110 - val_accuracy: 0.9554\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1672 - accuracy: 0.9750 - val_loss: 0.1790 - val_accuracy: 0.9696\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1399 - accuracy: 0.9800 - val_loss: 0.1561 - val_accuracy: 0.9757\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1198 - accuracy: 0.9950 - val_loss: 0.1392 - val_accuracy: 0.9797\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1051 - accuracy: 0.9950 - val_loss: 0.1266 - val_accuracy: 0.9838\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0938 - accuracy: 0.9950 - val_loss: 0.1163 - val_accuracy: 0.9858\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0847 - accuracy: 1.0000 - val_loss: 0.1065 - val_accuracy: 0.9888\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0762 - accuracy: 1.0000 - val_loss: 0.0999 - val_accuracy: 0.9899\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0704 - accuracy: 1.0000 - val_loss: 0.0939 - val_accuracy: 0.9899\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0649 - accuracy: 1.0000 - val_loss: 0.0888 - val_accuracy: 0.9899\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0603 - accuracy: 1.0000 - val_loss: 0.0839 - val_accuracy: 0.9899\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0559 - accuracy: 1.0000 - val_loss: 0.0802 - val_accuracy: 0.9899\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0525 - accuracy: 1.0000 - val_loss: 0.0768 - val_accuracy: 0.9899\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0496 - accuracy: 1.0000 - val_loss: 0.0738 - val_accuracy: 0.9899\n"
     ]
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "# Train for 4 epochs with 4 frozen layers\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "# Next 16 epochs unfreeze the recycle layers\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhbKiJ-NEqgw"
   },
   "source": [
    "So, what's the final verdict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i-IIokIuEqgx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 823us/step - loss: 0.1408 - accuracy: 0.9705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1408407837152481, 0.9704999923706055]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Te7FdtZEqgx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 836us/step - loss: 0.0682 - accuracy: 0.9935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0681881457567215, 0.9934999942779541]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fx8YvKAHEqgy"
   },
   "source": [
    "Great! We got quite a bit of transfer: the error rate dropped by a factor of 4!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6I-sRVmvEqgy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.214285714285701"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(100 - 97.05) / (100 - 99.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kd2gvdsTm4kk"
   },
   "source": [
    "It looks more impressive then it really is. \"I cheated! I tried many configurations until I found one that demonstrated a strong improvement. If you try to change the classes or the random seed, you will see that the improvement generally drops, or even vanishes or reverses. What I did is called “torturing the data until it confesses.”\"\n",
    "\n",
    "Whenever you see too impressive results, probably someone estimates 1000 models and is showing the best under some convinient justification. This is not nessarily wrong but is a major impediment in science replication.\n",
    "\n",
    "Recycling of layers works best with deep networks and esspecially with Convulutions Neural Networks, which we will study later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uuh6rcw4_gua"
   },
   "source": [
    "# Tweaking, Dropping, or Replacing the Upper Layers\n",
    "\n",
    "1. The output layer of the original model should usually be replaced since it is most likely not useful at all for the new task, and it may not even have the right number of outputs for the new task.\n",
    "2. the upper hidden layers of the original model are less likely to be as useful as the lower layers, since the high-level features that are most useful for the new task may differ significantly from the ones that were most useful for the original task.\n",
    "3. Try freezing all the copied layers first, then train your model and see how it performs.\n",
    "4. Then try unfreezing one or two of the top hidden layers to let backpropagation tweak them and see if performance improves. The more training data you have, the more layers you can unfreeze.\n",
    "5. If you still cannot get good performance, and you have little training data, try dropping the top hidden layer(s) and freeze all remaining hidden layers again.\n",
    "6. You can iterate until you find the right number of layers to reuse. If you have plenty of training data, you may try replacing the top hidden layers instead of dropping them, and even add more hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xG2XEqx5_gua"
   },
   "source": [
    "# Model Zoos\n",
    "Where can you find a neural network trained for a task similar to the one you want to tackle? The first place to look is obviously in your own catalog of models. This is one good reason to save all your models and organize them so you can retrieve them later easily. Another option is to search in a model zoo. Many people train Machine Learning models for various tasks and kindly release their pretrained models to the public."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXYgruBS_gub"
   },
   "source": [
    "# Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eKHpvFj-_gub"
   },
   "source": [
    "Training a very large deep neural network can be painfully slow. So far we have seen four ways to speed up training:\n",
    "\n",
    "1. applying a good initialization strategy for the connection weights\n",
    "2. using a good activation function\n",
    "3. using Batch Normalization/SELU\n",
    "4. reusing parts of a pretrained network. \n",
    "\n",
    "Another huge speed boost comes from using a faster optimizer than the regular Gradient Descent optimizer.\n",
    "\n",
    "\n",
    "Momentum optimization, Nesterov Accelerated Gradient, AdaGrad, RMSProp, and finally Adam and Nadam optimization.\n",
    " \n",
    "Adam optimization is usally the best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A8zUr6jW_gub"
   },
   "source": [
    "## Momentum optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xx5tWQQk_gub"
   },
   "source": [
    "Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal velocity (if there is some friction or air resistance).\n",
    "\n",
    "In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom.\n",
    "\n",
    "Gradient Descent simply updates the weights $\\theta$ by directly subtracting the gradient of the cost function $J(\\theta)$ with regards to the weights $(\\nabla\\theta J(\\theta))$multiplied by the learning rate $\\eta$. The equation is: $\\theta=\\theta - \\eta \\nabla_{\\theta} \\theta J(\\theta)$. \n",
    "\n",
    "It does not care about what the earlier gradients were. If the local gradient is tiny, it goes very slowly.\n",
    "\n",
    "\n",
    "Momentum optimization cares a great deal about what previous gradients were: at each iteration, it adds the local gradient to the momentum vector $m$ (multiplied by the learning rate $\\eta$), and it updates the weights by simply subtracting this momentum vector.\n",
    "\n",
    "In other words, the gradient is used as an acceleration, not as a speed. To simulate some sort of friction mechanism and prevent the momentum from growing too large, the algorithm introduces a new hyperparameter $\\beta$, simply called the momentum, which must be set between 0 (high friction) and 1 (no friction). A typical momentum value is 0.9.\n",
    "\n",
    "$$m = \\beta m + \\eta \\nabla_{\\theta} \\theta J(\\theta)$$\n",
    "$$\\theta = \\theta - m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-bObth1T_gub"
   },
   "source": [
    "If the gradient remains constant then the maximum size of weight updates is\n",
    "$$m(1-\\beta)= \\eta \\nabla_{\\theta} \\theta J(\\theta)$$\n",
    "For $\\beta = 0.9$, we reach terminal velocity when $m = 10 * \\eta \\nabla_{\\theta} \\theta J(\\theta)$. It means that momentum optimization will be 10 times faster than the gradient descent. Momentum is especially fast when dealing with flat-gradients and it helps not to get stuck in the local optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j0G79Mbd_gub"
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2mSCioc0_guc"
   },
   "source": [
    "## Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26M_Vs4V_guc"
   },
   "source": [
    "Modification of Momentum optimization, proposed by Yurii Nesterov in 1983, is almost always faster than vanilla Momentum optimization.\n",
    "\n",
    "The idea of Nesterov Momentum optimization, or Nesterov Accelerated Gradient (NAG) measures the gradient of the cost function not at the local position but slightly ahead in the direction of the momentum. The only difference from vanilla Momentum optimization is that the gradient is measured at $\\theta + \\beta m$ rather than at $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6cg04kBq_guc"
   },
   "source": [
    "$$m = \\beta m - \\eta \\nabla_{\\theta} \\theta J(\\theta + \\beta m)$$\n",
    "$$\\theta = \\theta + m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6wv8fhh_guc"
   },
   "source": [
    "This small tweak works because in general the momentum vector will be pointing in the right direction (i.e., toward the optimum), so it will be slightly more accurate to use the gradient measured a bit farther in that direction rather than using the gradient at the original position,\n",
    "\n",
    "\n",
    "$\\nabla_1$ is the gradient of the cost function measured at the starting point $\\theta$, and $\\nabla_2$ is the gradient at the point located at $\\theta + \\beta m$). \n",
    "\n",
    "The Nesterov update ends up slightly closer to the optimum. After a while, these small improvements add up and NAG ends up being significantly faster than regular Momentum optimization. \n",
    "\n",
    "Moreover, note that when the momentum pushes the weights across a valley, $\\nabla_1$ continues to push further across the valley, while $\\nabla_2$ pushes back toward the bottom of the valley. This helps reduce oscillations and thus converges faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZYvhxaDD_guc"
   },
   "source": [
    "![Neuron inner working g](images/Lesson10/image2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TgNg3p6c_gud"
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FH1HxOLv_gud"
   },
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ilk7Z27z_gud"
   },
   "source": [
    "Consider the elongated bowl problem again: Gradient Descent starts by quickly going down the steepest slope, then slowly goes down the bottom of the valley. \n",
    "\n",
    "It would be nice if the algorithm could detect this early on and correct its direction to point a bit more toward the global optimum. The AdaGrad algorithm achieves this by scaling down the gradient vector along the steepest dimensions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FlrHBMRS_gud"
   },
   "source": [
    "$$s=s + \\nabla_{\\theta} \\theta J(\\theta) \\otimes \\nabla_{\\theta} \\theta J(\\theta)$$\n",
    "$$\\theta = \\theta - \\eta \\nabla_{\\theta} \\theta J(\\theta) \\oslash \\sqrt{s + \\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-MzG84v_gue"
   },
   "source": [
    "The first step accumulates the square of the gradients into the vector $s$ (the $\\otimes$  symbol represents the element-wise multiplication). \n",
    "\n",
    "This vectorized form is equivalent to computing $s_i = s_i + (\\partial/\\partial \\theta_i J(\\theta))^2$ for each element $s_i$ of the vector $s$. \n",
    "Each $s_i$ accumulates the squares of the partial derivative of the cost function with regards to parameter $\\theta_i$. If the cost function is steep along the ith dimension, then $s_i$ will get larger and larger at each iteration.\n",
    "\n",
    "The second step is almost identical to Gradient Descent, but with one big difference: the gradient vector $s$ is scaled down by a factor of $\\sqrt{s+\\epsilon}$. $\\oslash$ is the element-by-element division, and $\\epsilon$ is the smoothing term to avoid division by zero set to $10^{-10}$. This is equivalent to element-wise:\n",
    "$\\theta_i = \\theta_i - \\eta \\partial/\\partial \\theta_i J(\\theta)/\\sqrt{s+\\epsilon}$\n",
    "\n",
    "\n",
    "In short, this algorithm decays the learning rate, but it does so faster for steep dimensions than for dimensions with gentler slopes. This is called an adaptive learning rate. It helps point the resulting updates more directly toward the global optimum  One additional benefit is that it requires much less tuning of the learning rate hyper-parameter $\\eta$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-AXduH9w_gue"
   },
   "source": [
    "![Neuron inner working g](images/Lesson10/image3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Vj6jxt-_gue"
   },
   "source": [
    "AdaGrad often performs well for simple quadratic problems, but unfortunately it often stops too early when training neural networks. The learning rate gets scaled down so much that the algorithm ends up stopping entirely before reaching the global optimum. So even though TensorFlow has an AdagradOptimizer, you should not use it to train deep neural networks (it may be efficient for simpler tasks such as Linear Regression, though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJK52YBw_gue"
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adagrad(lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRCGUmq0_guf"
   },
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DKXPHmQc_guf"
   },
   "source": [
    "Although AdaGrad slows down a bit too fast and ends up never converging to the global optimum, the RMSProp algorithm fixes this by accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training).\n",
    "\n",
    "It does so by using exponential decay in the first step:\n",
    "\n",
    "$$s=\\beta s + (1-\\beta) \\nabla_{\\theta} \\theta J(\\theta) \\otimes \\nabla_{\\theta} \\theta J(\\theta)$$\n",
    "$$\\theta = \\theta - \\eta \\nabla_{\\theta} \\theta J(\\theta) \\oslash \\sqrt{s + \\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxhXSQfA_guf"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RLAGlw7K_guf"
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Li1S0ye_gug"
   },
   "source": [
    "## Adam Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gHHqrR64_gug"
   },
   "source": [
    "Adam is Adaptive Moment Estimation, combines the ideas of Momentum optimization and RMSProp:\n",
    "1.Like Momentum optimization it keeps track of an exponentially decaying average of past gradients (direction)\n",
    "2.Like RMSProp it keeps track of an exponentially decaying average of past squared gradients (size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-UV5Sv3C_gug"
   },
   "source": [
    "1. $m = \\beta_1 m + (1- \\beta_1) \\nabla_{\\theta} J(\\theta)$\n",
    "2. $s= \\beta_2 s + (1 - \\beta_2) \\nabla_{\\theta} \\theta J(\\theta) \\otimes \\nabla_{\\theta} \\theta J(\\theta)$\n",
    "3. $m = \\frac{m}{1-\\beta_1^T}$\n",
    "4. $s = \\frac{s}{1-\\beta_2^T}$\n",
    "5. $\\theta = \\theta - \\eta m \\oslash \\sqrt{s + \\epsilon}$\n",
    "\n",
    "Steps 1,2 and 5 are similar to Momentum optimization and RMSProp.\n",
    "\n",
    "Steps 3 and 4 are technical: since $m$ and $s$ are initialized at 0, they will be biased toward $0$ at the beginning of training, so these two steps will help boost $m$ and $s$ at the beginning of training.\n",
    "\n",
    "The momentum decay hyperparameter $\\beta_1$ is typically initialized to 0.9, while the scaling decay hyperparameter $\\beta_2$ is often initialized to 0.999. As earlier, the smoothing term $\\epsilon$ is usually initialized to a tiny number such as $10^{–7}$. These are the default values for Keras AdamOptimizer class, so you can simply use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iOS_MjYBtsgo"
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yR9cVYo8uAs5"
   },
   "source": [
    "Nadam\n",
    "\n",
    "Nadam optimization is Adam optimization plus the Nesterov trick, so it will\n",
    "often converge slightly faster than Adam. \n",
    "\n",
    "While adaptive optimization works great on some datasets it performs poorly on others, so you need to try several ways in estimating your model (Ashia C. Wilson et al, 2017).\n",
    "\n",
    "All the optimization algorithms discussed above are based on first-order partial\n",
    "derivatives (Jacobians). While in theory second-order deriviatives (Hessian matrix) exists and are amazing, in the practice the number of second derivitatives is $n^2$ per output vs $n$ first deriviatives, which makes them impractical to use to for model with many parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WytiyKkXx3wg"
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7sARWpLFxIfP"
   },
   "source": [
    "\n",
    "![Neuron inner working g](images/Table_11_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sLjl6I-d_gug"
   },
   "source": [
    "## Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I23zPOar_gug"
   },
   "source": [
    "Finding a good learning rate can be tricky. If you set it way too high, training may actually diverge. If you set it too low, training will eventually converge to the optimum, but it will take a very long time.\n",
    "\n",
    "If you set it slightly too high, it will make progress very quickly at first, but it will end up dancing around the optimum, never settling down (unless you use an adaptive learning rate optimization algorithm such as AdaGrad, RMSProp, or Adam, but even then it may take time to settle).\n",
    "\n",
    "If you have a limited computing budget, you may have to interrupt training before it has converged properly, yielding a suboptimal solution.\n",
    "\n",
    "\n",
    "![Neuron inner working g](images/Lesson10/image4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sd7-dMQ2_gug"
   },
   "source": [
    "You can try running the model several time for optimal learning rate. But you can do better:\n",
    "\n",
    "1. start with a high learning rate\n",
    "2. reduce it once it stops making fast progress\n",
    "\n",
    "These strategies are called learning schedules:\n",
    "1. Predetermined piecewise constant learning rate: set the learning rate to $\\eta_0 = 0.1$ at first, then to $\\eta_1 = 0.001$ after 50 epochs. Although this solution can work very well, it often requires fiddling around to figure out the right learning rates and when to use them.\n",
    "2. Exponential scheduling. Set the learning rate to a function of the iteration number t: $\\eta(t) = \\eta_0 + 10^{-t/r}$ This works great, but it requires tuning $\\eta_0$ and $r$. The learning rate will drop by a factor of 10 every $r$ steps.\n",
    "3. Power scheduling. Set the learning rate to $\\eta(t) = \\eta_0 (1 + t/r)^{-c}$. The hyperparameter $c$ is typically set to 1. This is similar to exponential scheduling, but the learning rate drops much more slowly.\n",
    "4. Performance scheduling. Measure the validation error every $N$ steps (just like for early stopping), and reduce the learning rate by a factor of λ when the error stops dropping.\n",
    "5. 1cycle scheduling. 1cycle starts by increasing the initial learning rate $\\eta_0$, growing linearly up to $\\eta_1$ halfway through training. Then it decreases the learning rate linearly down to $\\eta_1$ again during the second half of training, finishing the last few epochs by dropping the rate down by several orders of magnitude (still linearly). The estimation invovles a bit more complex programming, so I won't cover in a great details, but I will be happy to answer the quesitons.\n",
    "\n",
    "The maximum learning rate $\\eta_1$ is chosen using the same approach we used to find the optimal learning rate, and the initial learning rate $\\eta_1$ is chosen to be roughly 10 times lower. \n",
    "\n",
    "When using a momentum, we start with a high momentum first (e.g., 0.95), then drop it down to a lower momentum during the first half of training\n",
    "(e.g., down to 0.85, linearly), and then bring it back up to the maximum value\n",
    "(e.g., 0.95) during the second half of training, finishing the last few epochs with that maximum value. Experiments showed that this approach\n",
    "was often able to speed up training considerably and reach better performance.\n",
    "For example, on the popular CIFAR10 image dataset, this approach reached\n",
    "91.9% validation accuracy in just 100 epochs, instead of 90.3% accuracy in 800\n",
    "epochs through a standard approach (with the same neural network\n",
    "architecture).\n",
    "\n",
    "Testing showed that oth performance scheduling and exponential scheduling  performed the best, but power is easy and fast. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "biIBvN6BMQSk"
   },
   "source": [
    "### Power Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "beBFCXOMMQSk"
   },
   "source": [
    "```lr = lr0 / (1 + steps / s)**c```\n",
    "* Keras uses `c=1` and `s = 1 / decay`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "doeDGQT5MQSk"
   },
   "outputs": [],
   "source": [
    "# specialized decay power scheduling\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YaMfvYmQMQSl"
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "vihQmylcMQSm",
    "outputId": "06b22e5a-33a6-4fa6-db87-5139b6914b7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4875 - accuracy: 0.8279 - val_loss: 0.4029 - val_accuracy: 0.8606\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3804 - accuracy: 0.8655 - val_loss: 0.3723 - val_accuracy: 0.8702\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 2s 995us/step - loss: 0.3484 - accuracy: 0.8769 - val_loss: 0.3750 - val_accuracy: 0.8680\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 2s 995us/step - loss: 0.3280 - accuracy: 0.8839 - val_loss: 0.3528 - val_accuracy: 0.8784\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3134 - accuracy: 0.8889 - val_loss: 0.3471 - val_accuracy: 0.8766\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2996 - accuracy: 0.8941 - val_loss: 0.3469 - val_accuracy: 0.8790\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2899 - accuracy: 0.8974 - val_loss: 0.3411 - val_accuracy: 0.8778\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 2s 987us/step - loss: 0.2810 - accuracy: 0.9012 - val_loss: 0.3439 - val_accuracy: 0.8778\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2728 - accuracy: 0.9031 - val_loss: 0.3326 - val_accuracy: 0.8846\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 2s 990us/step - loss: 0.2657 - accuracy: 0.9057 - val_loss: 0.3302 - val_accuracy: 0.8826\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2598 - accuracy: 0.9082 - val_loss: 0.3314 - val_accuracy: 0.8850\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 2s 991us/step - loss: 0.2539 - accuracy: 0.9110 - val_loss: 0.3369 - val_accuracy: 0.8800\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 2s 999us/step - loss: 0.2485 - accuracy: 0.9127 - val_loss: 0.3277 - val_accuracy: 0.8868\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2444 - accuracy: 0.9134 - val_loss: 0.3312 - val_accuracy: 0.8844\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 2s 969us/step - loss: 0.2396 - accuracy: 0.9156 - val_loss: 0.3266 - val_accuracy: 0.8846\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 2s 987us/step - loss: 0.2356 - accuracy: 0.9173 - val_loss: 0.3249 - val_accuracy: 0.8870\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 2s 975us/step - loss: 0.2318 - accuracy: 0.9183 - val_loss: 0.3272 - val_accuracy: 0.8842\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 2s 959us/step - loss: 0.2280 - accuracy: 0.9200 - val_loss: 0.3241 - val_accuracy: 0.8862\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 2s 967us/step - loss: 0.2247 - accuracy: 0.9217 - val_loss: 0.3262 - val_accuracy: 0.8852\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 2s 958us/step - loss: 0.2216 - accuracy: 0.9229 - val_loss: 0.3243 - val_accuracy: 0.8846\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 2s 978us/step - loss: 0.2180 - accuracy: 0.9241 - val_loss: 0.3252 - val_accuracy: 0.8854\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 2s 960us/step - loss: 0.2155 - accuracy: 0.9250 - val_loss: 0.3218 - val_accuracy: 0.8864\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 2s 975us/step - loss: 0.2127 - accuracy: 0.9258 - val_loss: 0.3248 - val_accuracy: 0.8870\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 2s 971us/step - loss: 0.2098 - accuracy: 0.9272 - val_loss: 0.3226 - val_accuracy: 0.8876\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 2s 958us/step - loss: 0.2077 - accuracy: 0.9282 - val_loss: 0.3228 - val_accuracy: 0.8864\n"
     ]
    }
   ],
   "source": [
    "# estimate baseline model\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "id": "PXBGU0jBMQSm",
    "outputId": "688a95a6-66f0-4998-efd8-5335a8adfaf6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initial learning rate\n",
    "learning_rate = 0.01\n",
    "# decay power\n",
    "decay = 1e-4\n",
    "# increase batch size to 32\n",
    "batch_size = 32\n",
    "# steps per epoch N/batch_size\n",
    "n_steps_per_epoch = len(X_train) // batch_size\n",
    "epochs = np.arange(n_epochs)\n",
    "# the learning rate function\n",
    "lrs = learning_rate / (1 + decay * epochs * n_steps_per_epoch)\n",
    "# Show a picture\n",
    "plt.plot(epochs, lrs,  \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.01])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Power Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CyF_Q63RMQSn"
   },
   "source": [
    "### Exponential Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GF87-tywMQSn"
   },
   "source": [
    "```lr = lr0 * 0.1**(epoch / s)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eM9nFzpMMQSn"
   },
   "outputs": [],
   "source": [
    "# Define exponential decay as function of the epoch\n",
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1**(epoch / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZBjdHZC9MQSo"
   },
   "outputs": [],
   "source": [
    "# add starting value\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_-ylL0LJMQSo"
   },
   "outputs": [],
   "source": [
    "# build a model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "GAmdNk5NMQSp",
    "outputId": "2c279f89-f1df-4ac1-9040-31691ea5be33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.8552 - accuracy: 0.7545 - val_loss: 0.8343 - val_accuracy: 0.7316\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.7179 - accuracy: 0.7869 - val_loss: 0.6313 - val_accuracy: 0.8084\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.6397 - accuracy: 0.8134 - val_loss: 0.7200 - val_accuracy: 0.7724\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5781 - accuracy: 0.8319 - val_loss: 0.5548 - val_accuracy: 0.8272\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.5254 - accuracy: 0.8423 - val_loss: 0.5734 - val_accuracy: 0.8300\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.4546 - accuracy: 0.8605 - val_loss: 0.5911 - val_accuracy: 0.8616\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.4376 - accuracy: 0.8655 - val_loss: 0.5290 - val_accuracy: 0.8604\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3882 - accuracy: 0.8781 - val_loss: 0.5041 - val_accuracy: 0.8596\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3696 - accuracy: 0.8827 - val_loss: 0.5030 - val_accuracy: 0.8516\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 4s 3ms/step - loss: 0.3366 - accuracy: 0.8901 - val_loss: 0.4428 - val_accuracy: 0.8784\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3087 - accuracy: 0.8979 - val_loss: 0.4230 - val_accuracy: 0.8760\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2866 - accuracy: 0.9023 - val_loss: 0.4847 - val_accuracy: 0.8704\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2668 - accuracy: 0.9091 - val_loss: 0.4950 - val_accuracy: 0.8826\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2489 - accuracy: 0.9132 - val_loss: 0.4550 - val_accuracy: 0.8748\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2291 - accuracy: 0.9188 - val_loss: 0.4557 - val_accuracy: 0.8798\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2145 - accuracy: 0.9253 - val_loss: 0.4832 - val_accuracy: 0.8820\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1979 - accuracy: 0.9301 - val_loss: 0.4941 - val_accuracy: 0.8794\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1874 - accuracy: 0.9341 - val_loss: 0.5007 - val_accuracy: 0.8844\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1728 - accuracy: 0.9392 - val_loss: 0.5399 - val_accuracy: 0.8884\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1598 - accuracy: 0.9428 - val_loss: 0.5401 - val_accuracy: 0.8858\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1501 - accuracy: 0.9474 - val_loss: 0.5741 - val_accuracy: 0.8862\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1407 - accuracy: 0.9505 - val_loss: 0.5717 - val_accuracy: 0.8806\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1318 - accuracy: 0.9545 - val_loss: 0.5654 - val_accuracy: 0.8870\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1247 - accuracy: 0.9567 - val_loss: 0.6071 - val_accuracy: 0.8846\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1164 - accuracy: 0.9597 - val_loss: 0.6068 - val_accuracy: 0.8832\n"
     ]
    }
   ],
   "source": [
    "# run the mode. Put user-define function through LearningRate scheduler\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "id": "oG2ocL_bMQSp",
    "outputId": "5a2a7d5d-cbf2-4932-afe2-d99f625bd11c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The decrease is much sharper than in the case of Power scheduling\n",
    "plt.plot(history.epoch, history.history[\"lr\"], \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.011])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Exponential Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "utgbRE4HMQSq"
   },
   "source": [
    "The schedule function can take the current learning rate as a second argument:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rtbf0kwjMQSs"
   },
   "source": [
    "### Piecewise Constant Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VwC_yIuAMQSs"
   },
   "outputs": [],
   "source": [
    "# define changes to the learning rate\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGO0CuMQMQSt"
   },
   "outputs": [],
   "source": [
    "# function is designed for 3 piecewise schedules\n",
    "def piecewise_constant(boundaries, values):\n",
    "    boundaries = np.array([0] + boundaries)\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch):\n",
    "        return values[np.argmax(boundaries > epoch) - 1]\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "-Yejk8ryMQSt",
    "outputId": "20e5cc85-1467-40d1-9c03-9f7125984bc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.8450 - accuracy: 0.7580 - val_loss: 0.8775 - val_accuracy: 0.7430\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.8240 - accuracy: 0.7643 - val_loss: 1.0066 - val_accuracy: 0.7138\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.8003 - accuracy: 0.7711 - val_loss: 1.9246 - val_accuracy: 0.6668\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.9649 - accuracy: 0.7050 - val_loss: 0.8715 - val_accuracy: 0.7570\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.8858 - accuracy: 0.7151 - val_loss: 1.1756 - val_accuracy: 0.5628\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.8425 - accuracy: 0.6789 - val_loss: 0.6874 - val_accuracy: 0.7276\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5994 - accuracy: 0.8062 - val_loss: 0.6197 - val_accuracy: 0.8210\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5739 - accuracy: 0.8199 - val_loss: 0.6870 - val_accuracy: 0.7720\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5401 - accuracy: 0.8429 - val_loss: 0.5998 - val_accuracy: 0.8044\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5016 - accuracy: 0.8503 - val_loss: 0.6034 - val_accuracy: 0.8564\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4828 - accuracy: 0.8544 - val_loss: 0.7800 - val_accuracy: 0.8376\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5085 - accuracy: 0.8596 - val_loss: 0.5188 - val_accuracy: 0.8534\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4611 - accuracy: 0.8671 - val_loss: 0.6982 - val_accuracy: 0.8280\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4848 - accuracy: 0.8629 - val_loss: 0.5628 - val_accuracy: 0.8466\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4722 - accuracy: 0.8605 - val_loss: 0.6768 - val_accuracy: 0.8478\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3343 - accuracy: 0.8948 - val_loss: 0.5437 - val_accuracy: 0.8708\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3009 - accuracy: 0.9041 - val_loss: 0.5241 - val_accuracy: 0.8698\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2823 - accuracy: 0.9092 - val_loss: 0.5302 - val_accuracy: 0.8772\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2704 - accuracy: 0.9138 - val_loss: 0.5089 - val_accuracy: 0.8762\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2613 - accuracy: 0.9169 - val_loss: 0.5119 - val_accuracy: 0.8784\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2528 - accuracy: 0.9200 - val_loss: 0.5431 - val_accuracy: 0.8810\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2331 - accuracy: 0.9259 - val_loss: 0.5163 - val_accuracy: 0.8884\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2121 - accuracy: 0.9310 - val_loss: 0.5336 - val_accuracy: 0.8812\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1979 - accuracy: 0.9344 - val_loss: 0.4843 - val_accuracy: 0.8864\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1894 - accuracy: 0.9373 - val_loss: 0.5318 - val_accuracy: 0.8834\n"
     ]
    }
   ],
   "source": [
    "# insert user-defined function\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "id": "6stHuL-IMQSu",
    "outputId": "4ba094a7-90fe-4e05-aa1a-df7e35d5d714"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.epoch, [piecewise_constant_fn(epoch) for epoch in history.epoch], \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.011])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Piecewise Constant Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N_z36OL7MQSu"
   },
   "source": [
    "### Performance Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNsG_xYrMQSu"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "8jdEVFXzMQSv",
    "outputId": "0c9ea597-fb2d-474c-bb53-663ca3aa7a4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5917 - accuracy: 0.8073 - val_loss: 0.4869 - val_accuracy: 0.8478\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 2s 962us/step - loss: 0.4973 - accuracy: 0.8394 - val_loss: 0.5958 - val_accuracy: 0.8270\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5334 - accuracy: 0.8373 - val_loss: 0.4869 - val_accuracy: 0.8584\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5156 - accuracy: 0.8466 - val_loss: 0.4588 - val_accuracy: 0.8548\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5144 - accuracy: 0.8512 - val_loss: 0.6096 - val_accuracy: 0.8300\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5111 - accuracy: 0.8541 - val_loss: 0.5359 - val_accuracy: 0.8498\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5334 - accuracy: 0.8531 - val_loss: 0.5457 - val_accuracy: 0.8522\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5429 - accuracy: 0.8532 - val_loss: 0.6445 - val_accuracy: 0.8218\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5542 - accuracy: 0.8516 - val_loss: 0.5472 - val_accuracy: 0.8560\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 2s 989us/step - loss: 0.3057 - accuracy: 0.8955 - val_loss: 0.3826 - val_accuracy: 0.8876\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 2s 1000us/step - loss: 0.2530 - accuracy: 0.9097 - val_loss: 0.4025 - val_accuracy: 0.8876\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2347 - accuracy: 0.9145 - val_loss: 0.4540 - val_accuracy: 0.8694\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 2s 967us/step - loss: 0.2193 - accuracy: 0.9202 - val_loss: 0.4310 - val_accuracy: 0.8866\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2060 - accuracy: 0.9242 - val_loss: 0.4406 - val_accuracy: 0.8814\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 2s 980us/step - loss: 0.1982 - accuracy: 0.9275 - val_loss: 0.4341 - val_accuracy: 0.8840\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1390 - accuracy: 0.9466 - val_loss: 0.4220 - val_accuracy: 0.8932\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1244 - accuracy: 0.9517 - val_loss: 0.4409 - val_accuracy: 0.8948\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 2s 987us/step - loss: 0.1161 - accuracy: 0.9545 - val_loss: 0.4480 - val_accuracy: 0.8898\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.1101 - accuracy: 0.9572 - val_loss: 0.4610 - val_accuracy: 0.8932\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 2s 976us/step - loss: 0.1031 - accuracy: 0.9598 - val_loss: 0.4845 - val_accuracy: 0.8918\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0834 - accuracy: 0.9689 - val_loss: 0.4829 - val_accuracy: 0.8934\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0783 - accuracy: 0.9708 - val_loss: 0.4906 - val_accuracy: 0.8952\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 2s 976us/step - loss: 0.0745 - accuracy: 0.9721 - val_loss: 0.4951 - val_accuracy: 0.8950\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.0706 - accuracy: 0.9733 - val_loss: 0.5109 - val_accuracy: 0.8948\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 2s 964us/step - loss: 0.0679 - accuracy: 0.9751 - val_loss: 0.5241 - val_accuracy: 0.8936\n"
     ]
    }
   ],
   "source": [
    "# We internal ReduceLROnPlateau function, which reduces learning rate by 0.5 is if it not improving for 5 iterations. \n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "optimizer = keras.optimizers.SGD(lr=0.02, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "id": "Hqi6Iu3dMQSv",
    "outputId": "d0c2c3e7-6608-4d5e-a732-ef6ccb1a64b4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LR on PLateau is a quicker, but less stablele learning. It reached optimimum by epoch 15 when overfitting started.\n",
    "# It also offered the fastest learning 6-7 seconds per epoch in the beginning.\n",
    "plt.plot(history.epoch, history.history[\"lr\"], \"bo-\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\", color='b')\n",
    "plt.tick_params('y', colors='b')\n",
    "plt.gca().set_xlim(0, n_epochs - 1)\n",
    "plt.grid(True)\n",
    "\n",
    "ax2 = plt.gca().twinx()\n",
    "ax2.plot(history.epoch, history.history[\"val_loss\"], \"r^-\")\n",
    "ax2.set_ylabel('Validation Loss', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "          \n",
    "plt.title(\"Reduce LR on Plateau\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "26Xrl97iMQSx"
   },
   "source": [
    "### 1Cycle scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QwMhKbQIqjfZ"
   },
   "source": [
    "This is the fastest method so far. The code is slightly more complicated because it uses class: funciton of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ISVh7jAyMQSx"
   },
   "outputs": [],
   "source": [
    "# call Keras\n",
    "K = keras.backend\n",
    "# this function saves in a vectors learning rates and losses and recalculates the new learning rate.\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "      # arguments to enter factor\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "      # get value of the current learning rate\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        # append the vector of losses\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        # set new value by multiplay factor by the current loss\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
    "\n",
    "\n",
    "# specify max and min rates\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "  # initial model weights\n",
    "    init_weights = model.get_weights()\n",
    "    # number of iteractions in the estimation\n",
    "    iterations = len(X) // batch_size * epochs\n",
    "    # factors growing linearly from max_rate to min_rate\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    # get learning rate\n",
    "    init_lr = K.get_value(model.optimizer.lr)\n",
    "    # start with initial learning rate\n",
    "    K.set_value(model.optimizer.lr, min_rate)\n",
    "    #save learning rates and losses in a vector\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    # estimate model with the new learning rate\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    # set value at initial rate\n",
    "    K.set_value(model.optimizer.lr, init_lr)\n",
    "    # assing initial raters\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "# plot resulting learning\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZI5_oy3YMQSy"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "# compile model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "colab_type": "code",
    "id": "peH7m0tRMQSy",
    "outputId": "6ad31289-5916-4a09-9ef7-8eb9e17f8ba5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430/430 [==============================] - 1s 2ms/step - loss: nan - accuracy: 0.3859\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "# run model savings raters and losses\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lC58L4KzMQSz"
   },
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "            rate = max(rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.lr, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "9W9YpbQ2MQSz",
    "outputId": "37e39213-2f83-4a77-993e-5f3a6339ec74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.6572 - accuracy: 0.7740 - val_loss: 0.4872 - val_accuracy: 0.8338\n",
      "Epoch 2/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.4580 - accuracy: 0.8397 - val_loss: 0.4274 - val_accuracy: 0.8520\n",
      "Epoch 3/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.4121 - accuracy: 0.8545 - val_loss: 0.4116 - val_accuracy: 0.8588\n",
      "Epoch 4/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.3837 - accuracy: 0.8642 - val_loss: 0.3868 - val_accuracy: 0.8688\n",
      "Epoch 5/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.3639 - accuracy: 0.8719 - val_loss: 0.3766 - val_accuracy: 0.8688\n",
      "Epoch 6/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.3456 - accuracy: 0.8775 - val_loss: 0.3739 - val_accuracy: 0.8706\n",
      "Epoch 7/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.3330 - accuracy: 0.8811 - val_loss: 0.3635 - val_accuracy: 0.8708\n",
      "Epoch 8/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.3184 - accuracy: 0.8861 - val_loss: 0.3959 - val_accuracy: 0.8610\n",
      "Epoch 9/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.3065 - accuracy: 0.8890 - val_loss: 0.3475 - val_accuracy: 0.8770\n",
      "Epoch 10/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.2943 - accuracy: 0.8927 - val_loss: 0.3392 - val_accuracy: 0.8806\n",
      "Epoch 11/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.2838 - accuracy: 0.8963 - val_loss: 0.3467 - val_accuracy: 0.8800\n",
      "Epoch 12/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.2707 - accuracy: 0.9024 - val_loss: 0.3646 - val_accuracy: 0.8696\n",
      "Epoch 13/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.2536 - accuracy: 0.9079 - val_loss: 0.3350 - val_accuracy: 0.8842\n",
      "Epoch 14/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.2405 - accuracy: 0.9135 - val_loss: 0.3465 - val_accuracy: 0.8794\n",
      "Epoch 15/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.2279 - accuracy: 0.9185 - val_loss: 0.3257 - val_accuracy: 0.8830\n",
      "Epoch 16/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.2159 - accuracy: 0.9232 - val_loss: 0.3294 - val_accuracy: 0.8824\n",
      "Epoch 17/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.2062 - accuracy: 0.9263 - val_loss: 0.3333 - val_accuracy: 0.8882\n",
      "Epoch 18/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.1978 - accuracy: 0.9301 - val_loss: 0.3235 - val_accuracy: 0.8898\n",
      "Epoch 19/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.1892 - accuracy: 0.9337 - val_loss: 0.3233 - val_accuracy: 0.8906\n",
      "Epoch 20/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.1821 - accuracy: 0.9365 - val_loss: 0.3224 - val_accuracy: 0.8928\n",
      "Epoch 21/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.1752 - accuracy: 0.9400 - val_loss: 0.3220 - val_accuracy: 0.8908\n",
      "Epoch 22/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.1700 - accuracy: 0.9416 - val_loss: 0.3180 - val_accuracy: 0.8962\n",
      "Epoch 23/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.1655 - accuracy: 0.9438 - val_loss: 0.3187 - val_accuracy: 0.8940\n",
      "Epoch 24/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.1627 - accuracy: 0.9454 - val_loss: 0.3177 - val_accuracy: 0.8932\n",
      "Epoch 25/25\n",
      "430/430 [==============================] - 1s 2ms/step - loss: 0.1610 - accuracy: 0.9462 - val_loss: 0.3170 - val_accuracy: 0.8934\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "onecycle = OneCycleScheduler(len(X_train) // batch_size * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eNCL_dUqvWqu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OKZ2yW6Vvag8"
   },
   "source": [
    "## $\\ell_1$ and $\\ell_2$ regularization\n",
    "\n",
    "You can use ℓ2 regularization to constrain a neural network’s connection weights, and/or ℓ1 regularization if you want a sparse model (with many weights equal to 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J660AjJ3MQS0"
   },
   "outputs": [],
   "source": [
    "# l2 regularization with alpha = 0.01\n",
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "# or l1(0.1) for ℓ1 regularization with a factor or 0.1\n",
    "# or l1_l2(0.1, 0.01) for both ℓ1 and ℓ2 regularization, with factors 0.1 and 0.01 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "lhb-GBnIMQS1",
    "outputId": "959573e2-da41-4ffa-da68-b1bbd7b47b8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 1.5956 - accuracy: 0.8124 - val_loss: 0.7169 - val_accuracy: 0.8340\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.7197 - accuracy: 0.8274 - val_loss: 0.6850 - val_accuracy: 0.8376\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(100, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(10, activation=\"softmax\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "oB8PL9c4MQS1",
    "outputId": "862f62b6-2748-4f4d-b0c6-c9ff8ca90872"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 1.6313 - accuracy: 0.8113 - val_loss: 0.7218 - val_accuracy: 0.8310\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.7187 - accuracy: 0.8273 - val_loss: 0.6826 - val_accuracy: 0.8382\n"
     ]
    }
   ],
   "source": [
    "# same code when we specified the desired layer and then loop over them\n",
    "from functools import partial\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBogq333_guj"
   },
   "source": [
    "# Avoiding Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hoR415fb_guq"
   },
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_bqmDSM_guq"
   },
   "source": [
    "The most popular regularization technique for deep neural networks is arguably dropout. It was proposed by G. E. Hinton in 2012 and further detailed in a paper by Nitish Srivastava et al., and it has proven to be highly successful: even the state-of-the-art neural networks got a 1–2% accuracy boost simply by adding dropout. <br>\n",
    "\n",
    "At every training step, every neuron (including the input neurons but excluding the output neurons) has a probability $p$ of being temporarily “dropped out,” meaning it will be entirely ignored during this training step, but it may be active during the next step. The hyperparameter $p$ is called the dropout rate, and it is typically set at 10%-50%. Usually, 20-30% in recurrent neural networks and 40-50% in convolutional neural networks. \n",
    "\n",
    "After training, neurons don’t get dropped anymore. And that’s all (except for a technical detail we will discuss momentarily).\n",
    "\n",
    "Imagine randomly 50% of employees are not working every day, then the remaining employees neeed to from new connections and revise their work finding new ways to do things. Dropping out is making companies and NN more resilient. \n",
    "![Neuron inner working g](images/image5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "_X-02ebQ_guq",
    "outputId": "7b088298-4742-4a3b-a8e6-034247d9f1bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5699 - accuracy: 0.8038 - val_loss: 0.3663 - val_accuracy: 0.8650\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4227 - accuracy: 0.8446 - val_loss: 0.3418 - val_accuracy: 0.8712\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3926 - accuracy: 0.8548 - val_loss: 0.3397 - val_accuracy: 0.8732\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3724 - accuracy: 0.8615 - val_loss: 0.3211 - val_accuracy: 0.8806\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3609 - accuracy: 0.8655 - val_loss: 0.3080 - val_accuracy: 0.8866\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3500 - accuracy: 0.8702 - val_loss: 0.3172 - val_accuracy: 0.8818\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3433 - accuracy: 0.8727 - val_loss: 0.3272 - val_accuracy: 0.8822\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3355 - accuracy: 0.8746 - val_loss: 0.3301 - val_accuracy: 0.8822\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3264 - accuracy: 0.8793 - val_loss: 0.3058 - val_accuracy: 0.8904\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3185 - accuracy: 0.8807 - val_loss: 0.3054 - val_accuracy: 0.8862\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3160 - accuracy: 0.8807 - val_loss: 0.3034 - val_accuracy: 0.8940\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3089 - accuracy: 0.8831 - val_loss: 0.3072 - val_accuracy: 0.8884\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3069 - accuracy: 0.8855 - val_loss: 0.3051 - val_accuracy: 0.8906\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3044 - accuracy: 0.8865 - val_loss: 0.3130 - val_accuracy: 0.8890\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2983 - accuracy: 0.8883 - val_loss: 0.3102 - val_accuracy: 0.8902\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2945 - accuracy: 0.8898 - val_loss: 0.3160 - val_accuracy: 0.8890\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2961 - accuracy: 0.8896 - val_loss: 0.3144 - val_accuracy: 0.8882\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2897 - accuracy: 0.8925 - val_loss: 0.3059 - val_accuracy: 0.8934\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2876 - accuracy: 0.8933 - val_loss: 0.2941 - val_accuracy: 0.8956\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2849 - accuracy: 0.8930 - val_loss: 0.2996 - val_accuracy: 0.8934\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "# 20% drop out rate\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 20\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "x8rWm1pPMQS3",
    "outputId": "d8e60231-beb4-408b-dbea-6ded297ee82b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 702us/step - loss: 0.3278 - accuracy: 0.8830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32775866985321045, 0.8830000162124634]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "xsCCRFtoMQS4",
    "outputId": "dadf3d7d-22d5-40ab-9743-01cebc70ded3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 1s 594us/step - loss: 0.1898 - accuracy: 0.9252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18981856107711792, 0.9251999855041504]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZVikx9HeMQS5"
   },
   "source": [
    "## MC Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PQmg_aYj2hOu"
   },
   "source": [
    "In 2016, a paper by Yarin Gal and Zoubin Ghahramani added a few more good reasons to use dropout:\n",
    "• First, the paper established a profound connection between dropout networks and approximate Bayesian inference, giving dropout a solid mathematical justification.\n",
    "• Second, the authors introduced a powerful technique called MC Dropout, which\n",
    "can boost the performance of any trained dropout model without having to\n",
    "retrain it or even modify it at all, provides a much better measure of the model’s uncertainty, and is also amazingly simple to implement.\n",
    "\n",
    "If this all sounds like a “one weird trick” advertisement, then take a look at the following\n",
    "code. It is the full implementation of MC Dropout, boosting the dropout model\n",
    "we trained earlier without retraining it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tw5WTbZDMQS5"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oORYqNvuMQS5"
   },
   "outputs": [],
   "source": [
    "y_probas = np.stack([model(X_test_scaled, training=True)\n",
    "                     for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "y_std = y_probas.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o3bJsIJl3G8y"
   },
   "source": [
    "We just make 100 predictions over the test set, setting training=True to ensure that the Dropout layer is active, and stack the predictions. \n",
    "\n",
    "Since dropout is active, all the predictions will be different. \n",
    "\n",
    "Recall that predict() returns a matrix with one row per instance and one column per class. Because there are 10,000 instances in the test set and 10 classes, this is a matrix of shape [10000, 10]. \n",
    "\n",
    "We stack 100 such matrices, so y_probas is an array of shape [100, 10000, 10]. Once we average over the first dimension (axis=0), we get y_proba, an array of shape [10000, 10], like we would get\n",
    "with a single prediction. \n",
    "\n",
    "\n",
    "That’s all! Averaging over multiple predictions with dropout on gives us a Monte Carlo estimate that is generally more reliable than the result of a\n",
    "single prediction with dropout off. For example, let’s look at the model’s prediction for the first instance in the Fashion MNIST test set, with dropout off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "aVbza48lMQS6",
    "outputId": "9ba4be22-df18-4dfe-db8b-0b6b38b7f895"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.29, 0.  , 0.69]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(X_test_scaled[:1]), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oyZpHW_h5E44"
   },
   "source": [
    "The model seems almost certain that this image belongs to class 9 (ankle boot).\n",
    "Should you trust it? Is there really so little room oom for doubt? Compare this with the\n",
    "predictions made when dropout is activated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fhzFR5vRMQS6",
    "outputId": "3fb7cd50-6c30-4742-9eb8-a3607067693c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.62, 0.  , 0.35]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.8 , 0.  , 0.19]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.17, 0.  , 0.75]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.13, 0.  , 0.84]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.75, 0.  , 0.22]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.71, 0.  , 0.19]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.54, 0.  , 0.36]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.68, 0.  , 0.32]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.45, 0.  , 0.55]],\n",
       "\n",
       "       [[0.  , 0.01, 0.  , 0.  , 0.  , 0.2 , 0.  , 0.19, 0.  , 0.6 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.37, 0.  , 0.62]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.23, 0.  , 0.77]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.44, 0.  , 0.48]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.62, 0.  , 0.36]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.11, 0.  , 0.79]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.48, 0.  , 0.35, 0.  , 0.18]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.1 , 0.  , 0.89]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.6 , 0.  , 0.39]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.78, 0.  , 0.2 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.38, 0.  , 0.6 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.21, 0.  , 0.79]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.32, 0.  , 0.38, 0.  , 0.3 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.39, 0.  , 0.54]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.2 , 0.  , 0.26, 0.  , 0.55]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.64, 0.  , 0.35]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.16, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.29, 0.  , 0.71]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.78, 0.  , 0.21]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.29, 0.  , 0.7 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.36, 0.  , 0.64]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.06, 0.  , 0.93]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.28, 0.  , 0.6 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.3 , 0.  , 0.68]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.48, 0.  , 0.5 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.3 , 0.  , 0.66]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.94]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.74, 0.  , 0.21]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.22, 0.  , 0.75]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.82, 0.  , 0.16]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.78, 0.  , 0.21]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.18, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.15, 0.  , 0.12, 0.  , 0.74]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.24, 0.  , 0.58, 0.  , 0.18]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.17, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.47, 0.  , 0.52]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.28, 0.  , 0.68]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.51, 0.  , 0.48]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.58, 0.  , 0.41]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.76, 0.  , 0.18]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.4 , 0.  , 0.55]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.29, 0.  , 0.69]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.41, 0.  , 0.59]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.16, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.43, 0.  , 0.56]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.43, 0.  , 0.45]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.52, 0.  , 0.41]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.68, 0.  , 0.31]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.49, 0.  , 0.46]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.25, 0.  , 0.68]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.64, 0.  , 0.32]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.08, 0.  , 0.79]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.51, 0.  , 0.42]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.72, 0.  , 0.18]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.52, 0.  , 0.46]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.59, 0.  , 0.33]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.14, 0.  , 0.85]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.03, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.71, 0.  , 0.25]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.02, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.63, 0.  , 0.37]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.43, 0.  , 0.57]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.18, 0.  , 0.78]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.82, 0.  , 0.14]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.79, 0.  , 0.2 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.35, 0.  , 0.64]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.23, 0.  , 0.64]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.4 , 0.  , 0.59]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.52, 0.  , 0.35]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.2 , 0.  , 0.79]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.16, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.29, 0.  , 0.7 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.33, 0.  , 0.67]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.17, 0.  , 0.4 , 0.  , 0.43]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.86]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.15, 0.  , 0.73]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.28, 0.  , 0.66]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.12, 0.  , 0.5 , 0.  , 0.38]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.44, 0.  , 0.55]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.92]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.7 , 0.  , 0.24]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_probas[:, :1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MSKbdcp38FeY"
   },
   "source": [
    "This tells a very different story: apparently, when we activate dropout, the model is not sure anymore. It still seems to prefer class 9, but sometimes it hesitates with classes 5 (sandal) and 7 (sneaker), which makes sense given they’re all footwear. Once we average over the first dimension, we get the following MC Dropout predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "bC4y1e7PMQS7",
    "outputId": "2e7ebc0d-3668-4c78-aa26-5e59f670f2a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.37, 0.  , 0.58]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_proba[:1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZKfNmOS18SUD"
   },
   "source": [
    "The model still thinks this image belongs to class 9, but only with a 78% confidence, which seems much more reasonable than 99%. Plus it’s useful to know exactly which other classes it thinks are likely. And you can also take a look at the standard deviation of the probability estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "nYQUzNiQMQS8",
    "outputId": "39aa9440-7271-420e-98e4-d4d2159e3936"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.24, 0.  , 0.26]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_std = y_probas.std(axis=0)\n",
    "np.round(y_std[:1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H2798DIH8dZT"
   },
   "source": [
    "Apparently there’s quite a lot of variance in the probability estimates: if you were building a risk-sensitive system (e.g., a medical or financial system), you should probably treat such an uncertain prediction with extreme caution. You definitely would not treat it like a 99% confident prediction. Moreover, the model’s accuracy got a small boost from 86.8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VJWExrX3MQS8"
   },
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QB0OpmZxMQS9",
    "outputId": "abd01dae-2a79-49de-c1db-3495d8a04fc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8844"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P6co13qT8zHZ"
   },
   "source": [
    "If your model contains other layers that behave in a special way during training (such as BatchNormalization layers), then you should not force training mode like we just did. Instead, you should replace the Dropout layers with the following MCDropout\n",
    "class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h1kUtBivMQS9"
   },
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ata9uhS0MQS-"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ewwhy6DeMQS-"
   },
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "6lOLWNhjMQS_",
    "outputId": "6c0a2673-134f-456e-d633-8f06a4c9e7cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJvB-xwaMQS_"
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "mc_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IoOnWEz6MQTA"
   },
   "outputs": [],
   "source": [
    "mc_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2hAbwNfCMQTA"
   },
   "source": [
    "Now we can use the model with MC Dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "wQ5UjEAgMQTA",
    "outputId": "9348ae38-cc21-4f74-c69c-5bb808e72b9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.29, 0.  , 0.69]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.mean([mc_model.predict(X_test_scaled[:1]) for sample in range(100)], axis=0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ROgwyjHg9EqE"
   },
   "source": [
    "Here, we just subclass the Dropout layer and override the call() method to force its training argument to True (see Chapter 12). If you are creating a model from scratch, it’s just a matter of using MCDropout rather than Dropout.\n",
    "\n",
    "But if you have a model that was already trained using Dropout, you need to create a new model that’s identical to the existing model except that it replaces the Dropout layers with MCDrop out, then copy the existing model’s weights to your new model.\n",
    "\n",
    "In short, MC Dropout is a fantastic technique that boosts dropout models and providesbetter uncertainty estimates. And of course, since it is just regular dropout during training, it also acts like a regularizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vhxr_9IOMQTB"
   },
   "source": [
    "## Max norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g7SOIRgjMQTB"
   },
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                           kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "_wQ-P3X0MQTC",
    "outputId": "ef58f86e-abe9-4141-fadc-4911a8ed685e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4740 - accuracy: 0.8323 - val_loss: 0.3674 - val_accuracy: 0.8674\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3543 - accuracy: 0.8716 - val_loss: 0.3714 - val_accuracy: 0.8662\n"
     ]
    }
   ],
   "source": [
    "MaxNormDense = partial(keras.layers.Dense,\n",
    "                       activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       kernel_constraint=keras.constraints.max_norm(1.))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    MaxNormDense(300),\n",
    "    MaxNormDense(100),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0MBEIbkI_gut"
   },
   "source": [
    "If you observe that the model is overfitting, you can increase the dropout rate. Conversely, you should try decreasing the dropout rate if the model underfits the training set.\n",
    "\n",
    "It can also help to increase the dropout rate for large layers, and reduce it for small ones. \n",
    "\n",
    "Dropout does tend to significantly slow down convergence, but it usually results in a much better model when tuned properly. So, it is generally well worth the extra time and effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yZeUy-Na_gut"
   },
   "source": [
    "Cheat Sheet:\n",
    "\n",
    "Initialization : He\n",
    "\n",
    "Activation Function: ELE/SELU\n",
    "\n",
    "Normalization: Batch/None\n",
    "\n",
    "Regularization: Dropout\n",
    "\n",
    "Optimizer: Adam\n",
    "\n",
    "Learning Rate Schedule: None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JOy31pmz_gut"
   },
   "source": [
    "1. If you can’t find a good learning rate (convergence was too slow, so you increased the training rate, and now convergence is fast but the network’s accuracy is suboptimal), then you can try adding a learning schedule such as exponential decay.\n",
    "\n",
    "2. If your training set is a bit too small, you can implement data augmentation.\n",
    "\n",
    "3. If you need a sparse model, you can add some ℓ1 regularization to the mix (and optionally zero out the tiny weights after training). \n",
    "4. If you need an even sparser model, you can try using FTRL instead of Adam optimization, along with ℓ1 regularization.\n",
    "5. If you need a lightning-fast model at runtime, you may want to drop Batch Normalization, and possibly replace the ELU activation function with the leaky ReLU. H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vOQQo5gL9r2C"
   },
   "source": [
    "![Neuron inner working g](images/Table_11_3.png)\n",
    "![Neuron inner working g](images/Table_11_4.png)\n",
    "\n",
    "* Don’t forget to normalize the input features!\n",
    "* features! You should also try to reuse parts of a\n",
    "pretrained neural network if you can find one that solves a similar problem\n",
    "* If you need a low-latency model (one that performs lightning-fast predictions),\n",
    "you may need to use fewer layers, fold the Batch Normalization layers into the\n",
    "previous layers, and possibly use a faster activation function such as leaky ReLU\n",
    "or just ReLU. Finally, you may want to\n",
    "reduce the float precision from 32 bits to 16 or even 8 bits\n",
    "* If you are building a risk-sensitive application, or inference latency is not very\n",
    "important in your application, you can use MC Dropout to boost performance\n",
    "and get more reliable probability estimates, along with uncertainty estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2pTXdypZ9twj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ZwEaMi50_gtt",
    "hnyFUTDYEqga",
    "l6KKVUWSEqgj",
    "-45KUQu4Eqgj",
    "Uuh6rcw4_gua",
    "xG2XEqx5_gua",
    "A8zUr6jW_gub",
    "2mSCioc0_guc",
    "FH1HxOLv_gud",
    "kRCGUmq0_guf",
    "0Li1S0ye_gug",
    "CyF_Q63RMQSn",
    "eBogq333_guj",
    "oU_U89Ya_guj",
    "hoR415fb_guq",
    "bOFUgs-i_gut",
    "OHJXre7x_gu0",
    "uJuzdWII_gu1",
    "egDtVRqa_gu1",
    "S1Kyv46L_gu1",
    "X1CUXV3g_gu2",
    "n8_GFwRA_gu4",
    "y00Qxazi_gu9",
    "CK-22hKQ_gvD",
    "QBdRfWfa_gvH",
    "FVRHF2dU_gvH",
    "pN9FvgHV_gvJ",
    "jPNHa_SY_gvL",
    "k1r1AUO6_gvT",
    "r58ob4wM_gvV",
    "YKKUhMXs_gvY",
    "EhdpptPe_gvZ",
    "Oov_0oNe_gvg",
    "TPI45leq_gvj",
    "62Nx8lUm_gvl"
   ],
   "name": "Lecture 8b Deep Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "nav_menu": {
   "height": "360px",
   "width": "416px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
